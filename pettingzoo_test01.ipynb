{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "官网石头剪刀布 AEC示例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class raw_env(AECEnv):\n",
    "    \"\"\"\n",
    "    The metadata holds environment constants. From gymnasium, we inherit the \"render_modes\",\n",
    "    metadata which specifies which modes can be put into the render() method.\n",
    "    At least human mode should be supported.\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"官网石头剪刀布并行示例_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment argumen官网石头剪刀布并行示例and\n",
    "         should define the following attributes:\n",
    "        - possible_agents\n",
    "        - render_mode\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "\n",
    "        # optional: a mapping between agent name and ID\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "        # optional: we can define the observation and action spaces here as attributes to be used in their corresponding methods\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(4) for agent in self.possible_agents\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Observe should return the observation of the specified agent. This function\n",
    "        should return a sane observation (though not necessarily the most up to date possible)\n",
    "        at any time after reset() is called.\n",
    "        \"\"\"\n",
    "        # observation of one agent is the previous state of the other\n",
    "        return np.array(self.observations[agent])\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the following attributes\n",
    "        - agents\n",
    "        - rewards\n",
    "        - _cumulative_rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection\n",
    "        And must set up the environment so that render(), step(), and observe()\n",
    "        can be called without issues.\n",
    "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: NONE for agent in self.agents}\n",
    "        self.observations = {agent: NONE for agent in self.agents}\n",
    "        self.num_moves = 0\n",
    "        \"\"\"\n",
    "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \"\"\"\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for the current agent (specified by\n",
    "        agent_selection) and needs to update\n",
    "        - rewards\n",
    "        - _cumulative_rewards (accumulating the rewards)\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection (to the next agent)\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            # handles stepping an agent which is already dead\n",
    "            # accepts a None action for the one agent, and moves the agent_selection to\n",
    "            # the next dead agent,  or if there are no more dead agents, to the next live agent\n",
    "            self._was_dead_step(action)\n",
    "            return\n",
    "\n",
    "        agent = self.agent_selection\n",
    "\n",
    "        # the agent which stepped last had its _cumulative_rewards accounted for\n",
    "        # (because it was returned by last()), so the _cumulative_rewards for this\n",
    "        # agent should start again at 0\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "\n",
    "        # stores action of current agent\n",
    "        self.state[self.agent_selection] = action\n",
    "\n",
    "        # collect reward if it is the last agent to act\n",
    "        if self._agent_selector.is_last():\n",
    "            # rewards for all agents are placed in the .rewards dictionary\n",
    "            self.rewards[self.agents[0]], self.rewards[self.agents[1]] = REWARD_MAP[\n",
    "                (self.state[self.agents[0]], self.state[self.agents[1]])\n",
    "            ]\n",
    "\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "\n",
    "            # observe the current state\n",
    "            for i in self.agents:\n",
    "                self.observations[i] = self.state[\n",
    "                    self.agents[1 - self.agent_name_mapping[i]]\n",
    "                ]\n",
    "        else:\n",
    "            # necessary so that observe() returns a reasonable observation at all times.\n",
    "            self.state[self.agents[1 - self.agent_name_mapping[agent]]] = NONE\n",
    "            # no rewards are allocated until both players give an action\n",
    "            self._clear_rewards()\n",
    "\n",
    "        # selects the next agent.\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        # Adds .rewards to ._cumulative_rewards\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "\n",
    "# import aec_rps\n",
    "# env = aec_rps.env(render_mode=\"human\")\n",
    "env = env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "官网石头剪刀布并行示例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Game over\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import ParallelEnv\n",
    "from pettingzoo.utils import parallel_to_aec, wrappers\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def raw_env(render_mode=None):\n",
    "    \"\"\"\n",
    "    To support the AEC API, the raw_env() function just uses the from_parallel\n",
    "    function to convert from a ParallelEnv to an AEC env\n",
    "    \"\"\"\n",
    "    env = parallel_env(render_mode=render_mode)\n",
    "    env = parallel_to_aec(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class parallel_env(ParallelEnv):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and should define the following attributes:\n",
    "        - possible_agents\n",
    "        - render_mode\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "\n",
    "        # optional: a mapping between agent name and ID\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the `agents` attribute and must set up the\n",
    "        environment so that render(), and step() can be called without issues.\n",
    "        Here it initializes the `num_moves` variable which counts the number of\n",
    "        hands that are played.\n",
    "        Returns the observations for each agent\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.num_moves = 0\n",
    "        observations = {agent: NONE for agent in self.agents}\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "        self.state = observations\n",
    "\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for each agent and should return the\n",
    "        - observations\n",
    "        - rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        dicts where each dict looks like {agent_1: item_1, agent_2: item_2}\n",
    "        \"\"\"\n",
    "        # If a user passes in actions with no agents, then just return empty observations, etc.\n",
    "        if not actions:\n",
    "            self.agents = []\n",
    "            return {}, {}, {}, {}, {}\n",
    "\n",
    "        # rewards for all agents are placed in the rewards dictionary to be returned\n",
    "        rewards = {}\n",
    "        rewards[self.agents[0]], rewards[self.agents[1]] = REWARD_MAP[\n",
    "            (actions[self.agents[0]], actions[self.agents[1]])\n",
    "        ]\n",
    "\n",
    "        terminations = {agent: False for agent in self.agents}\n",
    "\n",
    "        self.num_moves += 1\n",
    "        env_truncation = self.num_moves >= NUM_ITERS\n",
    "        truncations = {agent: env_truncation for agent in self.agents}\n",
    "\n",
    "        # current observation is just the other player's most recent action\n",
    "        observations = {\n",
    "            self.agents[i]: int(actions[self.agents[1 - i]])\n",
    "            for i in range(len(self.agents))\n",
    "        }\n",
    "        self.state = observations\n",
    "\n",
    "        # typically there won't be any information in the infos, but there must\n",
    "        # still be an entry for each agent\n",
    "        infos = {agent: {} for agent in self.agents}\n",
    "\n",
    "        if env_truncation:\n",
    "            self.agents = []\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return observations, rewards, terminations, truncations, infos\n",
    "\n",
    "\n",
    "# 示例使用\n",
    "\n",
    "env2 = parallel_env(render_mode=\"human\")\n",
    "observations, infos = env2.reset()\n",
    "\n",
    "while env2.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env2.action_space(agent).sample() for agent in env2.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env2.step(actions)\n",
    "env2.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}