{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy和pytorch基本操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查看维度（形状）\n",
    "与numpy的np.shape()类似，torch通过.shape和.size()查看维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a.shape)\n",
    "print(a.size())\n",
    "print(np.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 另外，还可以获取其中的某一维度\n",
    "print(a.shape[0])\n",
    "print(a.shape[1])\n",
    "print(a.size(0))\n",
    "print(a.size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 3 2 1 1 2 3 4 5 6]\n",
      "\n",
      "\n",
      "[[1 2 3]\n",
      " [3 2 1]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "\n",
      "[[1 2 3 1 2 3]\n",
      " [3 2 1 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# append 可以拼接两个数组、或数组和值，但是效率低一些\n",
    "import numpy as np\n",
    "a = np.array([[1,2,3],[3,2,1]])\n",
    "b = np.array([[1,2,3], [4,5,6]])\n",
    "print(np.append(a,b))\n",
    "print('\\n')\n",
    "print(np.append(a,b,axis=0 ))\n",
    "print('\\n')\n",
    "print(np.append(a,b,axis=1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [3 2 1]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "\n",
      "[[1 2 3]\n",
      " [3 2 1]\n",
      " [1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "\n",
      "[[1 2 3 1 2 3]\n",
      " [3 2 1 4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# concatenate 只能拼接维度相同的数组（可以是多个），但是大规模效率更高\n",
    "print(np.concatenate((a,b)))\n",
    "print('\\n')\n",
    "print(np.concatenate((a,b),axis=0) )\n",
    "print('\\n')\n",
    "print(np.concatenate((a,b),axis=1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2 3]\n",
      "  [3 2 1]]\n",
      "\n",
      " [[1 2 3]\n",
      "  [4 5 6]]] \n",
      "\n",
      "[[[1 2 3]\n",
      "  [1 2 3]]\n",
      "\n",
      " [[3 2 1]\n",
      "  [4 5 6]]] \n",
      "\n",
      "[[[1 1]\n",
      "  [2 2]\n",
      "  [3 3]]\n",
      "\n",
      " [[3 4]\n",
      "  [2 5]\n",
      "  [1 6]]]\n"
     ]
    }
   ],
   "source": [
    "# stack 加维度的拼接\n",
    "# 此处省略hstack, vstack与dstack\n",
    "print( np.stack((a, b)),'\\n' )# 默认增加一个维度\n",
    "print( np.stack((a, b), axis=1),'\\n' )\n",
    "print( np.stack((a, b), axis=2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor拼接\n",
    "常用的方法有\n",
    "1. torch.cat\n",
    "2. torch.stack\n",
    "3. dstack、hstack、vstack、row_stack、column_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat方法\n",
    "torch.cat(tensors, dim=0, `*`, out=None) -> Tensor\n",
    "在指定的维度dim（默认是0）上面进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39404\\1140230739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 沿着dim=0拼接，dim1不变\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 沿着dim=1拼接，dim0不变\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# 不存在这个维度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "# 低维拼接\n",
    "import torch\n",
    "ones = torch.ones(4, 5)\n",
    "zeros = torch.zeros(4, 5)\n",
    "print(torch.cat((ones, zeros), dim=0))# 沿着dim=0拼接，dim1不变\n",
    "print(torch.cat((ones, zeros), dim=1))# 沿着dim=1拼接，dim0不变\n",
    "print(torch.cat((ones, zeros), dim=2))# 不存在这个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 4, 5])\n",
      "torch.Size([2, 6, 4, 5])\n",
      "torch.Size([2, 3, 8, 5])\n"
     ]
    }
   ],
   "source": [
    "# 高维度拼接\n",
    "ones = torch.ones(2, 3, 4, 5)\n",
    "zeros = torch.zeros(2, 3, 4, 5)\n",
    "cat1 = torch.cat((ones, zeros), dim=0)\n",
    "print(cat1.shape)\n",
    "cat2 = torch.cat((ones, zeros), dim=1)\n",
    "print(cat2.shape)\n",
    "cat3 = torch.cat((ones, zeros), dim=2)\n",
    "print(cat3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack方法\n",
    "torch.stack(tensors, dim=0, *, out=None) → Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 8])\n",
      "torch.Size([1, 2, 8])\n",
      "torch.Size([2, 1, 2, 8])\n",
      "torch.Size([2, 1, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0, 16).reshape(1, 2, 8)\n",
    "b = torch.arange(0, 16).reshape(1, 2, 8)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "c1 = torch.stack((a, b))\n",
    "print(c1.shape)\n",
    "a1 = torch.unsqueeze(a, dim=0)\n",
    "b1 = torch.unsqueeze(b, dim=0)\n",
    "c2 = torch.cat((a1, b1))\n",
    "print(c2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dstack、hstack、vstack、row_stack、column_stack\n",
    "\n",
    "torch.dstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "torch.hstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "torch.vstack(tensors, *, out=None) → Tensor\n",
    "\n",
    "torch.row_stack(tensors, *, out=None) → Tensor\n",
    "\n",
    "torch.column_stack(tensors, *, out=None) → Tensor\n",
    "\n",
    "这些拼接函数主要针对三维张量。即一个张量维度表示为行(Rows/horizontal)、列(Columns/virtical)和深度(Depth)时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 16])\n",
      "torch.Size([10, 10, 8])\n",
      "torch.Size([20, 5, 8])\n",
      "torch.Size([20, 5, 8])\n",
      "torch.Size([10, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "COLUMNS = 5\n",
    "ROWS = 10\n",
    "DEPTH = 8\n",
    "\n",
    "a1 = torch.arange(0, COLUMNS * ROWS * DEPTH).reshape(ROWS, COLUMNS, DEPTH)\n",
    "b1 = torch.arange(0, COLUMNS * ROWS * DEPTH).reshape(ROWS, COLUMNS, DEPTH)\n",
    "\n",
    "# 深度相加\n",
    "c1 = torch.dstack((a1, b1))\n",
    "print(c1.shape)  # torch.Size([10, 5, 16])\n",
    "\n",
    "# 水平相加（行相加）\n",
    "c2 = torch.hstack((a1, b1))\n",
    "print(c2.shape)  # torch.Size([10, 10, 8])\n",
    "\n",
    "# 垂直相加（列相加）\n",
    "c3 = torch.vstack((a1, b1))\n",
    "print(c3.shape)  # torch.Size([20, 5, 8])\n",
    "\n",
    "# 列相加\n",
    "c4 = torch.row_stack((a1, b1))\n",
    "print(c4.shape)  # torch.Size([20, 5, 8])\n",
    "\n",
    "# 行相加\n",
    "c5 = torch.column_stack((a1, b1))\n",
    "print(c5.shape)  # torch.Size([10, 10, 8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy升降维度\n",
    "\n",
    "https://blog.csdn.net/m0_50572604/article/details/120782788\n",
    "\n",
    "### numpy升维可以用下列方法\n",
    "1. np.atleast_2d(array) 转为二维数组\n",
    "2. np.atleast_3d(array) 转为三维数组\n",
    "3. array[:,np.newaxis] 升维一次 n行一列\n",
    "4. array[np.newaxis,:] 升维一次 一行n列\n",
    "5. array.reshape(-1,1) 变成n行一列\n",
    "6. array.reshape(1,-1) 变成一行n列\n",
    "7. np.expand_dims(a, axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n",
      "[[1 2 3 4 5]]\n",
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4,5])\n",
    "#将数组升为二维数组\n",
    "b = np.atleast_2d(a)\n",
    "#通过转置来改变二维数组的形状\n",
    "c = a.T\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b=np.expand_dims(a, axis=0)\n",
    "print(np.shape(b))\n",
    "c=np.expand_dims(a, axis=1)\n",
    "print(np.shape(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy降维可以用下列方法\n",
    "1. array.ravel()\n",
    "2. np.squeeze(array)\n",
    "3. array.reshape(-1)\n",
    "4. array.flatten（）：返回源数据的副本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5]])\n",
    "b=np.squeeze(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pytorch升降维度\n",
    "https://blog.csdn.net/poisonchry/article/details/121042431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch升维用torch.unsqueeze(input, dim) → Tensor\n",
    "其中，dim 的范围是`[-input.dim()-1, input.dim()+1)`,也就是允许用户以顺序、逆序的方式插入维度。举例来说，如果`dim=-1`，张量的维度会从`(A*B)`变为`(A*B*1)`;如果 `dim=0`，维度会变成`(1*A*B)`；如果`dim=1`，会变成`(A*1*B)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(torch.unsqueeze(x, 0))\n",
    "print(torch.unsqueeze(x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch降维用torch.squeeze(input, dim=None, `*`, out=None) → Tensor\n",
    "维度压缩，如果指定的dim参数下维度为1，会删除指定的维度，否则不变；如果没指定dim参数，会把张量中所有为1的维度全部删除，以此达到降维操作。如果输入维度是`(A*B*1*C*D*1)`，会输出维度`(A*B*C*D)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(2, 1, 2, 1, 2)\n",
    "print(x.size())\n",
    "y = torch.squeeze(x)# 维度全降\n",
    "print(y.size())\n",
    "y1 = torch.squeeze(x, 0)#无效降维\n",
    "print(y1.size())\n",
    "y2 = torch.squeeze(x, 1)#有效降维\n",
    "print(y2.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy和tensor互转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "x = np.ones(5)\n",
    "print(type(x)) # 查看x的类型\n",
    "'numpy 转 tensor'\n",
    "x1 = torch.tensor(x)\n",
    "print(type(x1))\n",
    "x2 = torch.from_numpy(x)\n",
    "print(type(x2))\n",
    "'tensor转numpy'\n",
    "y = torch.ones(5) # 创建张量x\n",
    "# tensor([1., 1., 1., 1., 1.])\n",
    "y_ = y.detach().numpy() # 转换\n",
    "print(type(y_))\n",
    "y__= y.numpy()\n",
    "print(type(y__))\n",
    "# 主要区别在于是否使用detach()，也就是返回的新变量是否需要计算梯度。【用了detach()，不需要计算梯度了】"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "\n",
    "numpy和pytorch都有广播机制，“广播”这一术语用于描述如何在形状不一的数组上应用算术运算。\n",
    "在满足特定限制的前提下，较小的数组“广播至”较大的数组，使两者形状互相兼容。广播提供了一个向量化数组操作的机制，这样遍历就发生在C层面，而不是Python层面。广播可以避免不必要的数据复制，通常导向高效的算法实现。不过，也存在不适用广播的情形（可能导致拖慢计算过程的低效内存使用）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/CANGYE0504/article/details/117353589\n",
    "https://blog.csdn.net/Lewiz_124/article/details/141597903\n",
    "\n",
    "numpy的广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'两个数组一样形状的时候就不用广播了'\n",
    "from numpy import array\n",
    "a = array([1.0, 2.0, 3.0])\n",
    "b = array([2.0, 2.0, 2.0])\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 4., 6.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'数组和标量相乘的时候会自动广播标量'\n",
    "b = 2.0\n",
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (2,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20932\\1022345824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (2,2) "
     ]
    }
   ],
   "source": [
    "# 示例1：相同形状的张量总是可广播的，因为总能满足以上规则。\n",
    "x = np.empty([5, 7, 3])\n",
    "y = np.empty([5, 7, 3])\n",
    "print((x+y).shape)\n",
    "# 示例2：不可广播（ a 不满足第一条规则）。\n",
    "a = np.empty([0])\n",
    "b = np.empty([2, 2])\n",
    "print((a+b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 4, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,2,4,1) (3,1,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20932\\2214593142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m   \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,2,4,1) (3,1,1) "
     ]
    }
   ],
   "source": [
    "# 示例3：m 和 n 可广播：\n",
    "m = np.empty([5, 3, 4, 1])\n",
    "n = np.empty([   3, 1, 1])\n",
    "\n",
    "# 倒数第一个维度：两者的尺寸均为1\n",
    "# 倒数第二个维度：n尺寸为1\n",
    "# 倒数第三个维度：两者尺寸相同\n",
    "# 倒数第四个维度：n该维度不存在\n",
    "print(((m+n).shape))\n",
    "\n",
    "# 示例4：不可广播，因为倒数第三个维度：2 != 3\n",
    "p = np.empty([5, 2, 4, 1])\n",
    "q = np.empty([   3, 1, 1])\n",
    "print(((p+q).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/86997775\n",
    "\n",
    "pytorch的广播机制\n",
    "\n",
    "- 可广播的一对张量需满足以下规则：\n",
    "1. 每个张量至少有一个维度。\n",
    "2. 迭代维度尺寸时，从尾部的维度开始，维度尺寸\n",
    "    1. 或者相等，\n",
    "    2. 或者其中一个张量的维度尺寸为 1 ，\n",
    "    3. 或者其中一个张量不存在这个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36760\\1395395780.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (0) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 示例1：相同形状的张量总是可广播的，因为总能满足以上规则。\n",
    "x = torch.empty(5, 7, 3)\n",
    "y = torch.empty(5, 7, 3)\n",
    "print((x+y).shape)\n",
    "\n",
    "# 示例2：不可广播（ a 不满足第一条规则）。\n",
    "a = torch.empty((0,))\n",
    "b = torch.empty(2, 2)\n",
    "print((a+b).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36760\\3571304760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m   \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 示例3：m 和 n 可广播：\n",
    "m = torch.empty(5, 3, 4, 1)\n",
    "n = torch.empty(   3, 1, 1)\n",
    "\n",
    "# 倒数第一个维度：两者的尺寸均为1\n",
    "# 倒数第二个维度：n尺寸为1\n",
    "# 倒数第三个维度：两者尺寸相同\n",
    "# 倒数第四个维度：n该维度不存在\n",
    "print((m+n).shape)\n",
    "\n",
    "# 示例4：不可广播，因为倒数第三个维度：2 != 3\n",
    "p = torch.empty(5, 2, 4, 1)\n",
    "q = torch.empty(   3, 1, 1)\n",
    "print((p+q).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36760\\779547067.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m   \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 示例5：可广播\n",
    "c = torch.empty(5, 1, 4, 1)\n",
    "d = torch.empty(   3, 1, 1)\n",
    "(c + d).size()  # torch.Size([5, 3, 4, 1])\n",
    "print((c + d).shape)\n",
    "\n",
    "# 示例6：可广播\n",
    "f = torch.empty(      1)\n",
    "g = torch.empty(3, 1, 7)\n",
    "(f + g).size()  # torch.Size([3, 1, 7])\n",
    "print((f + g).shape)\n",
    "\n",
    "# 示例7：不可广播\n",
    "o = torch.empty(5, 2, 4, 1)\n",
    "u = torch.empty(   3, 1, 1)\n",
    "print((o + u).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量矩阵操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy按位乘、点乘、叉乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 10 18]\n",
      " [18 10  4]] 按位乘\n",
      "\n",
      "[[ 4 10 18]\n",
      " [ 6 10 12]] 按位乘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "'按位乘只能数值/向量同矩阵、或是两个形状一样的矩阵相乘'\n",
    "a = np.array([[1,2,3],[3,2,1]])\n",
    "b = np.array([[4,5,6],[6,5,4]])\n",
    "print(a*b,'按位乘\\n')\n",
    "print(a[0]*b,'按位乘\\n')\n",
    "# 按位乘满足交换律"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32 28]\n",
      " [28 32]] 矩阵点乘\n",
      "\n",
      "[32 28] 矩阵和向量点乘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'点乘用np.dot'\n",
    "print(np.dot(a,b.T),'矩阵点乘\\n')\n",
    "print(np.dot(a[0],b.T),'矩阵和向量点乘\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3  6 -3] 向量叉乘\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'叉乘用np.cross'\n",
    "print(np.cross(a[0], b[0]), '向量叉乘\\n')\n",
    "# 二维向量叉乘出菱形面积，三维向量叉乘出来法向量\n",
    "# print(np.cross(a, b), '矩阵叉乘')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy求范数\n",
    "x_norm=np.linalg.norm(x, ord=None, axis=None, keepdims=False)\n",
    "\n",
    "向量范数：\n",
    "\n",
    "    ord默认=2计算2范数，ord=1算1范数、ord=np.inf取绝对值最大，还有-np.inf范数、p范数\n",
    "    \n",
    "矩阵范数：\n",
    "\n",
    "    ord=1：列和的最大值\n",
    "    ord=2：|λE-ATA|=0，求特征值，然后求最大特征值得算术平方根\n",
    "    ord=∞：行和的最大值\n",
    "    axis=1表示按行向量处理，求多个行向量的范数\n",
    "    axis=0表示按列向量处理，求多个列向量的范数\n",
    "    axis=None表示矩阵范数。\n",
    "    keepdims：是否保持矩阵的二维特性, True表示保持矩阵的二维特性，False相反"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  5  6  3 -1] \n",
      " [[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]]\n",
      "向量2范数:\n",
      "8.48528137423857\n",
      "8.48528137423857\n",
      "默认的矩阵范数:\n",
      "22.494443758403985\n",
      "矩阵2范数:\n",
      "22.409298163270435\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x1=np.array([1,5,6,3,-1])\n",
    "x2=np.arange(12).reshape(3,4)\n",
    "print(x1,'\\n',x2)\n",
    "print('向量2范数:')\n",
    "print(np.linalg.norm(x1))\n",
    "print(np.linalg.norm(x1,ord=2))\n",
    "print('默认的矩阵范数:')\n",
    "print(np.linalg.norm(x2))\n",
    "print('矩阵2范数:')\n",
    "print(np.linalg.norm(x2,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch按位乘、点乘、叉乘\n",
    "\n",
    "pytorch向量乘法和numpy差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  6, 12])\n",
      "tensor(20)\n",
      "tensor([-1,  2, -1])\n"
     ]
    }
   ],
   "source": [
    "'向量乘法'\n",
    "a=torch.tensor([1,2,3])\n",
    "b=torch.tensor([2,3,4])\n",
    "print(a*b) # 按位乘\n",
    "print(torch.dot(a,b)) # 向量点乘\n",
    "print(torch.cross(a,b)) # 向量叉乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch矩阵的乘法符号含义和numpy有些区别：\n",
    "```\n",
    "“点乘”为两个矩阵对应元素相乘（逐元素级element-wise）\n",
    "实现方式：可以通过*和torch.mul(x, y)函数实现（含广播机制）\n",
    "模型符号：一个圆圈中有一个实心点⊙\n",
    "\n",
    "“叉乘”为传统的线性代数学的矩阵乘法\n",
    "实现方式：可以通过torch.mm()和torch.matmul()实现（含广播机制）\n",
    "模型符号：一个圆圈中有一个叉×\n",
    "\n",
    "plus: 逐元素相加\n",
    "实现方式：可以通过+和torch.add(x, y)函数实现\n",
    "模型符号：一个圆圈中有一个加号＋\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*用法\n",
      "\n",
      "tensor([10, 40, 90])\n",
      "tensor([[ 10,  40,  90],\n",
      "        [120, 100,  60]]) \n",
      "\n",
      "mul用法\n",
      "\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "'按位乘*和torch.mul(a,b)（含广播机制）'\n",
    "import torch\n",
    "print('*用法\\n')\n",
    "a=torch.tensor([[1,2,3],[4,5,6]])\n",
    "b=torch.tensor([[10,20,30],[30,20,10]])\n",
    "print(a[0]*b[0])\n",
    "print(a*b,'\\n')\n",
    "print('mul用法\\n')\n",
    "a = torch.ones(3,4)\n",
    "print(a)\n",
    "b = torch.Tensor([1,2,3]).reshape((3,1))\n",
    "print(b)\n",
    "print((torch.mul(a, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "self must be a matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20932\\3062197896.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: self must be a matrix"
     ]
    }
   ],
   "source": [
    "'torch.mm()和torch.matmul()'\n",
    "# 实现方式：可以通过torch.mm()和torch.matmul()（含广播机制）实现\n",
    "x = torch.ones(3,4)\n",
    "y = torch.eye(4)          #对角线为1，其余元素为0\n",
    "print(torch.mm(x,y))\n",
    "z = torch.ones(5,4,3)\n",
    "print(torch.mm(z,x))# mm没有广播机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]],\n",
      "\n",
      "        [[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.matmul(z,x)) ## matmul有广播机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/ding_programmer/article/details/120822430\n",
    "补充：\n",
    "- 三维带batch的矩阵乘法 torch.bmm()\n",
    "\n",
    "由于神经网络训练一般采用mini-batch，经常输入的时三维带batch的矩阵，所以提供\n",
    "\n",
    "torch.bmm(bmat1, bmat2, out=None)\n",
    "\n",
    "- dot 函数：\n",
    "\n",
    "对于秩为1的数组，执行对应位置相乘，然后再相加，等价于向量的点乘；\n",
    "\n",
    "对于秩不为1的二维数组，执行矩阵乘法运算，等价于矩阵的叉乘；\n",
    "\n",
    "- `*` 和 `@`\n",
    "\n",
    "`*` 代表的是矩阵逐元素乘\n",
    "\n",
    "`@` 代表的是 矩阵乘法\n",
    "\n",
    "\n",
    "经验\n",
    "1. 把向量 看成 1*n 的矩阵,不过需要采用 unsqueeze() 函数来增加一下维度。转置的话 ，一般采用 a.t() 或者 a.transpose(0,1)\n",
    "\n",
    "2. 矩阵点乘 用  torch.mul\n",
    "\n",
    "3. 矩阵乘法 用 torch.matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "tensor([[ 7, 10],\n",
      "        [15, 22]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([[1,2],[3,4]])\n",
    "y=torch.tensor([[1,2],[3,4]])\n",
    "print(x*y)\n",
    "print(x@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch范数\n",
    "\n",
    "torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)\n",
    "返回所给定tensor的矩阵范数或向量范数,所谓范数也就是把一个高纬度的东西,压缩成为一个大于等于零的数,用以估算这里东西的大小(幅度)\n",
    "\n",
    "- input：输入tensor\n",
    "- p (int, float, inf, -inf, 'fro', 'nuc', optional)：范数计算中的幂指数值。默认为'fro'\n",
    "- dim (int，2-tuple，2-list， optional): 指定计算的维度。如果是一个整数值，向量范数将被计算；如果是一个大小为2的元组，矩阵范数将被计算；如果为None，当输入tensor只有两维时矩阵计算矩阵范数；当输入只有一维时则计算向量范数。如果输入tensor超过2维，向量范数将被应用在最后一维\n",
    "- keepdim（bool，optional）：指明输出tensor的维度dim是否保留。如果dim=None或out=None,则忽略该参数。默认值为False，不保留\n",
    "- out（Tensor, optional）:tensor的输出。如果dim=None或out=None,则忽略该参数。\n",
    "- dtype（torch.dtype，optional）：指定返回tensor的期望数据类型。如果指定了该参数，在执行该操作时输入tensor将被转换成 :attr:’dtype’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])\n",
      "tensor([[-4., -3., -2.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [ 2.,  3.,  4.]])\n",
      "tensor(7.7460)\n",
      "tensor(7.7460)\n",
      "tensor(4.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(9, dtype=torch.float) - 4\n",
    "b = a.reshape(3,3)\n",
    "print(a)\n",
    "print(b)\n",
    "print((torch.norm(a)))# 如果不指明p，则是计算Frobenius范数：\n",
    "print(torch.norm(b))\n",
    "print(torch.norm(a, float('inf'))) # p = 'inf',则是求出矩阵或向量中各项元素绝对值中的最大值，所以为4\n",
    "print(torch.norm(b, float('-inf')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [-1.,  1.,  4.]])\n",
      "tensor([1.4142, 2.2361, 5.0000])\n",
      "torch.Size([3])\n",
      "tensor([3.7417, 4.2426])\n",
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "c = torch.tensor([[1,2,3],[-1,1,4]], dtype=torch.float)\n",
    "print(c)\n",
    "print(torch.norm(c, dim=0)) # 指定dim = 0，因为c的size() = (2,3),所以会去掉其dim=0，得到size()=(3)的结果，所以是纵向求值，计算Frobenius范数\n",
    "print(torch.norm(c, dim=0).size())\n",
    "print(torch.norm(c, dim=1))\n",
    "print(torch.norm(c, p=1, dim=1)) # p=1, dim=1 ： 即是表示去掉维度1，使用1-范数，得到size()=(2)的结果。所以横向计算各个元素绝对值的和，为（[6,6]）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.7417, 11.2250])\n",
      "torch.Size([2, 2, 2])\n",
      "tensor([[4.0000, 5.0990],\n",
      "        [6.3246, 7.6158]])\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "torch.Size([2, 2])\n",
      "tensor(3.7417)\n",
      "tensor(11.2250)\n"
     ]
    }
   ],
   "source": [
    "d = torch.arange(8, dtype=torch.float).reshape(2,2,2)\n",
    "print(torch.norm(d, dim=(1,2)))\n",
    "print(d.size())\n",
    "print(torch.norm(d, dim=0))\n",
    "print(d[0,:,:])\n",
    "print(d[0,:,:].size())\n",
    "print(torch.norm(d[0,:,:]))\n",
    "print(torch.norm(d[1,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einsum爱因斯坦求和\n",
    "\n",
    "https://rockt.github.io/2018/04/30/einsum\n",
    "\n",
    "https://blog.csdn.net/ashome123/article/details/117110042\n",
    "\n",
    "该方法在numpy和pytorch中均有内置\n",
    "\n",
    "einsum方法正是利用了爱因斯坦求和简介高效的表示方法，从而可以驾驭任何复杂的矩阵计算操作。基本的框架如下：\n",
    "\n",
    "```C = einsum('ij,jk->ik', A, B)```\n",
    "\n",
    "上述操作表示矩阵A与矩阵B的点积。输入的参数分为两部分，前面表示计算操作的字符串，后面是以逗号隔开的操作对象（数量需与前面对应）。其中在计算操作表示中，\"->“左边是以逗号隔开的下标索引，重复出现的索引即是需要爱因斯坦求和的；”->\"右边的是最后输出的结果形式。\n",
    "\n",
    "以上式为例，其计算公式为：$C_{ik} = \\sum_jA_{ij}B_{jk}$，其等价于矩阵A与B的点积。\n",
    "这里有几条原则需要注意，之后也会和结合示例进行详解：\n",
    "1. \"->\"左边的是对应维度，以逗号隔开\n",
    "2. \"->\"右边的是最终output的形式\n",
    "3. 如果符号\"->\"被省略则代表输出为整体求和\n",
    "4. \"…\"表示省略之前或之后的所有维度\n",
    "5. einsum中涉及到的计算操作有很多，包括但不限于点积、对应元素相乘、求和、转置等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " tensor([[ 0.5498, -1.1671,  1.0803,  1.6470,  0.1439],\n",
      "        [ 0.5186, -1.1010,  1.0191,  1.5537,  0.1358],\n",
      "        [ 0.2870, -0.6092,  0.5639,  0.8597,  0.0751],\n",
      "        [ 0.1998, -0.4241,  0.3926,  0.5985,  0.0523],\n",
      "        [ 0.4313, -0.9156,  0.8476,  1.2922,  0.1129]]) \n",
      "\n",
      "2\n",
      " tensor([[4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4.]]) \n",
      "\n",
      "3\n",
      " tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) \n",
      "\n",
      "4\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]]]) \n",
      "\n",
      "5\n",
      " tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]]]) \n",
      "\n",
      "6\n",
      " tensor([[-1.4149, -1.3289,  0.8769],\n",
      "        [-1.4583,  0.0309, -0.7495]]) \n",
      "\n",
      "7\n",
      " tensor([[-1.2930,  3.5172,  2.7362, -2.6307],\n",
      "        [ 1.3358, -0.0562, -2.1890, -0.6088],\n",
      "        [-0.5720,  1.1801,  3.6130, -4.7107],\n",
      "        [ 0.3446,  0.4758, -0.2872,  0.3638],\n",
      "        [-2.0129, -3.0333,  1.8778, -4.2763],\n",
      "        [ 0.9917, -7.1611, -6.9570, -2.5521]]) \n",
      "\n",
      "tensor([[-1.2930,  3.5172,  2.7362, -2.6307],\n",
      "        [ 1.3358, -0.0562, -2.1890, -0.6088],\n",
      "        [-0.5720,  1.1801,  3.6130, -4.7107],\n",
      "        [ 0.3446,  0.4758, -0.2872,  0.3638],\n",
      "        [-2.0129, -3.0333,  1.8778, -4.2763],\n",
      "        [ 0.9917, -7.1611, -6.9570, -2.5521]])\n",
      "8\n",
      " \n",
      " tensor([[-1.2930,  3.5172,  2.7362, -2.6307],\n",
      "        [ 1.3358, -0.0562, -2.1890, -0.6088],\n",
      "        [-0.5720,  1.1801,  3.6130, -4.7107],\n",
      "        [ 0.3446,  0.4758, -0.2872,  0.3638],\n",
      "        [-2.0129, -3.0333,  1.8778, -4.2763],\n",
      "        [ 0.9917, -7.1611, -6.9570, -2.5521]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import einsum\n",
    "a = torch.ones(3,4)\n",
    "b = torch.ones(4,5)\n",
    "c = torch.ones(6,7,8)\n",
    "d = torch.ones(3,4)\n",
    "x, y = torch.randn(5), torch.randn(5)\n",
    "'计算矩阵所有元素之和'\n",
    "# einsum('i,j', a)   # 等价于einsum('i,j->', a)\n",
    "# einsum('i,j,k', c)\n",
    "'计算矩阵的迹'\n",
    "# einsum('ii', a)\n",
    "'获取矩阵对角线元素组成的向量'\n",
    "# einsum('ii->i', a)\n",
    "\"以上全都运行不了\"\n",
    "\n",
    "'向量相乘得到矩阵'\n",
    "print('1\\n',einsum('i,j->ij', x, y),'\\n')\n",
    "'矩阵点积'\n",
    "print('2\\n',einsum('ij,jk->ik', a, b),'\\n')\n",
    "'矩阵对应元素相乘'\n",
    "print('3\\n',einsum('ij,ij->ij', a, d),'\\n')\n",
    "'矩阵的转置'\n",
    "print('4\\n',einsum('ijk->ikj', c),'\\n')\n",
    "print('5\\n',einsum('...jk->...kj', c) ,'\\n') # 两种形式等价\n",
    "'双线性运算'\n",
    "A = torch.randn(3,5,4)\n",
    "l = torch.randn(2,5)\n",
    "r = torch.randn(2,4)\n",
    "print('6\\n',torch.einsum('bn,anm,bm->ba', l, A, r),'\\n')\n",
    "'最后来一个复杂的'\n",
    "a = torch.randn(3,4,5)\n",
    "b = torch.randn(6,5)\n",
    "c = torch.randn(6,3)\n",
    "target = einsum('fti,di,df->dt', a,b,c)\n",
    "print('7\\n',target,'\\n')\n",
    "# 把上面的求和过程写成循环的形式方便理解\n",
    "# 对f和i进行求和\n",
    "dt = torch.zeros(6,4)\n",
    "for d in range(6):\n",
    "    for t in range(4):\n",
    "        tmp = 0\n",
    "        for f in range(3):\n",
    "            for i in range(5):\n",
    "                tmp += a[f,t,i].item() * b[d,i].item() * c[d,f].item()\n",
    "        dt[d,t] = tmp\n",
    "\n",
    "print(target)\n",
    "print('8\\n','\\n', dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T20:25:34.134559Z",
     "start_time": "2024-05-14T20:25:34.112550Z"
    }
   },
   "source": [
    "# pytorch保存读取网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import rl_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方文档：\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=load_state_dict\n",
    "\n",
    "其他：\n",
    "https://blog.csdn.net/weixin_64388392/article/details/134587209\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   torch.save：将序列化对象保存至磁盘。可以保存模型、tensor、字典等对象，使用python中pickle模块进行序列化\n",
    "-   torch.load：使用pickle的unpickling功能将pickle对象反序列化至内存\n",
    "-   torch.nn.Moudle.load\\_state\\_dict：使用反序列化函数state\\_dict 来加载模型参数字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## state\\_dict字典状态\n",
    "\n",
    "torch.nn.Module模型中包含可学习参数，可用model.parameters()访问参数。state\\_dict是python字典对象，它将每一层映射到其参数张量，只有可学习参数的层的模型才具有state\\_dict这一项。目标优化torch.optim的优化参数对象也有state\\_dict属性，包含优化器相关信息，以及使用的超参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# 定义模型\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型\n",
    "model = TheModelClass()\n",
    "\n",
    "# 初始化优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 打印模型的状态字典\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# 打印优化器的状态字典\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载state\\_dict（推荐使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 保存\\ntorch.save(model.state_dict(),PATH)  \\n# 加载\\nmodel = TheModelClass(*args, **kwargs)\\nmodel.load_state_dict(torch.load(PATH))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存\n",
    "torch.save(model.state_dict(),'save_test')  \n",
    "# 加载\n",
    "# model = TheModelClass(*args, **kwargs) # NameError: name 'args' is not defined\n",
    "model.load_state_dict(torch.load('save_test'))\n",
    "\n",
    "'''\n",
    "# 保存\n",
    "torch.save(model.state_dict(),PATH)  \n",
    "# 加载\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   pytorch中常见保存模型格式'.pt'或'.pth'\n",
    "-   运行推理之前，需要调用model.eval()去设置dropout和batch normalization层为评估模型，如果不这么做，导致模型推断结果不一致\n",
    "-   load\\_state\\_dict()只接受字典对象，需要使用torch.load来反序列化state\\_dict模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载整个模型（不推荐）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33996\\3718694584.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 保存\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 加载\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存\n",
    "torch.save(model, PATH)\n",
    "# 加载\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此部分保存/加载过程使用最直观的语法并涉及最少量的代码。以 Python 'pickle' 模块的方式来保存模型。这种方法的缺点是序列化数据受 限于某种特殊的类而且需要确切的字典结构。这是因为pickle无法保存模型类本身。相反，它保存包含类的文件的路径，该文件在加载时使用。 因此，当在其他项目使用或者重构之后，您的代码可能会以各种方式中断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存/加载checkpoint用于推理或继续训练\n",
    "\n",
    "### 单个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型保存\n",
    "torch.save({\n",
    "'epoch':epoch, 'model_state_dict':model.state_dict(),\n",
    "'optimizer_state_dict':optimizer.state_dict(),\n",
    "'loss':loss,\n",
    "...\n",
    "},PATH)\n",
    "\n",
    "# 模型加载\n",
    "model = TheModelClass(args, **kwargs)\n",
    "optimizer = TheOptimizerClass(args, **kwargs)\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict')\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.train() or model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# 搭建网络模型\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 保存模型\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            ...\n",
    "            }, 'model_path')\n",
    "\n",
    "# 加载模型\n",
    "model = TheModelClass()# 实例化模型\n",
    "# 定义优化器（不唯一）\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    " \n",
    "# 加载模型\n",
    "checkpoint = torch.load('model_path')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种保存模型的方法不仅将模型以字典的形式进行了保存，同时还将模型迭代训练轮数：epoch、定义的优化器：otimizer、损失值：loss等都进行了保存，因此这种保存方法很有利于我们因各种原因而导致训练中止后继续恢复训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多个模型保存\n",
    "torch.save({\n",
    "'modelA_state_dict':modelA.state_dict(),\n",
    "'optimizerA_state_dict':optimizerA.state_dict(),\n",
    "'modelB_state_dict':modelB.state_dict(),\n",
    "'optimizerB_state_dict':optimizerB.state_dict(),\n",
    "}, PATH)\n",
    "\n",
    "# 多个模型加载\n",
    "modelA = TheModelAClass(args, **kwargs)\n",
    "modelB = TheModelBClass(args, *kwargs)\n",
    "optimizerA = TheOptimizerAClass(args, *kwargs)\n",
    "optimizerB = TheOptimizerBClass(args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
    "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
    "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
    "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
    "\n",
    "modelA.eval() or modelA.train()\n",
    "modelB.eval() or modelB.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同模型参数下热启动模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3522999687.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LYH\\AppData\\Local\\Temp\\ipykernel_33996\\3522999687.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    torch.save(model.state_dict(),PATH)Text only\u001b[0m\n\u001b[1;37m                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),PATH)Text only\n",
    "modelB = TheModelClass(*args. **kwargs)\n",
    "modelB.load_state_dict(torch.load(PATH), strict=False)  # strict 该参数指明是否需要强制严格匹配，默认True，strict=False 忽略非匹配键的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不同设备保存/加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU or GPU 上保存模型\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# CPU上保存，CPU上加载\n",
    "device = torch.device('cpu')\n",
    "model = TheModelClass(*args, kwargs)\n",
    "model.load_state_dict(torch.load(PATH, map_location=device)) # 使用map_location参量将张量下的存储器动态重新映射到CPU设备\n",
    "\n",
    "# CPU上保存，GPU上加载\n",
    "device = torch.device('cuda')\n",
    "model = TheModelClass()\n",
    "model.load_state_dict(torch.load(PATH, map_loaction='cuda:0')) # 将模型加载到指定的GPU设备上\n",
    "model.to(device) # 将模型的参数张量转换成CUDA张量\n",
    "input = input.to(device) # 模型输入装换成CUDA张量\n",
    "\n",
    "# GPU上保存，GPU上加载\n",
    "device = torch.device('cuda')\n",
    "model = TheModelClass()\n",
    "model = model.load_state_dict(torch.load(PATH))\n",
    "model.to(device) # 将初始化的模型转换成CUDA优化模型\n",
    "my_tensor = my_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存torch.nn.DataParallel模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.module.state_dict(),PATH)\n",
    "# torch.nn.DataParallel是一个模型封装，支持并行GPU使用。\n",
    "# 要普通保存 DataParallel 模型, 请保存model.module.state_dict()。 \n",
    "# 这样，你就可以非常灵活地以任何方式加载模型到你想要的设备中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN中第一次见的写法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{'a': 1, 'b': '3'}\n"
     ]
    }
   ],
   "source": [
    "# d = {key1 : value1, key2 : value2 }\n",
    "tinydict = {'a': 1, 'b': 2, 'b': '3'}\n",
    "print(tinydict['b'])\n",
    "print(tinydict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组。\n",
    "\n",
    "一个简单的字典实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tinydict = { 'abc': '456' }\n",
    "tinydict1 = { 'abc': 456 }\n",
    "tinydict2 = { 'abc': 123, 98.6: 37 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字典添加、修改、删除、清空和更新元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'美琳': 18, '梦洁': 19, '雪丽': 19, '美莲': 18, '梅梅': 20}\n",
      "{'美琳': 18, '梦洁': 20, '雪丽': 19, '美莲': 18, '梅梅': 20}\n",
      "{'美琳': 18, '雪丽': 19, '美莲': 18, '梅梅': 20}\n",
      "{'美琳': 18, '美莲': 18, '梅梅': 20}\n",
      "{'美琳': 18, '美莲': 18}\n"
     ]
    }
   ],
   "source": [
    "a = {'美琳': 18, '梦洁': 19, '雪丽': 19, '美莲': 18}\n",
    "# 添加\n",
    "a['梅梅'] = 20\n",
    "print(a)\n",
    "# 修改\n",
    "a['梦洁'] = 20\n",
    "print(a)\n",
    "# del删除\n",
    "del a['梦洁']\n",
    "print(a)\n",
    "# pop删除\n",
    "a.pop('雪丽')\n",
    "print(a)\n",
    "# 使用popitem() 方法删除字典中最后一个键值对\n",
    "a.popitem()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你要删除的键不在字典中\n",
      "原字典为： {'美琳': 18, '梦洁': 19, '雪丽': 19, '美莲': 18}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# 带判断删除\n",
    "a = {'美琳': 18, '梦洁': 19, '雪丽': 19, '美莲': 18}  # 指定一个原始字典\n",
    "if '梅梅' in a:                                       # 如果在字典中\n",
    "    del a['梅梅']                                     # 删除一个元素\n",
    "else:                                                 # 否则\n",
    "    print(\"你要删除的键不在字典中\")                   # 告诉结果\n",
    "print(\"原字典为：\",a)    \n",
    "# 清空\n",
    "a.clear()\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'美琳': 18, '梦洁': 20, '雪丽': 19, '美莲': 18, '梅梅': 18}\n"
     ]
    }
   ],
   "source": [
    "# 使用update 更新\n",
    "a = {'美琳': 18, '梦洁': 19, '雪丽': 19, '美莲': 18}\n",
    "a.update({'梅梅': 18, '梦洁': 20})         \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "字典的遍历"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小明\n",
      "小兰\n",
      "小红\n",
      "小明\n",
      "小兰\n",
      "小红\n",
      "小明 <class 'str'>\n",
      "小兰 <class 'str'>\n",
      "小红 <class 'str'>\n",
      "129 <class 'int'>\n",
      "148 <class 'int'>\n",
      "89 <class 'int'>\n",
      "同时遍历写法1\n",
      "key:  小明 value:  129\n",
      "key:  小兰 value:  148\n",
      "key:  小红 value:  89\n",
      "同时遍历写法2\n",
      "key:  小明 value:  129\n",
      "key:  小兰 value:  148\n",
      "key:  小红 value:  89\n",
      "同时遍历写法3\n",
      "kv is :  ('小明', 129)\n",
      "kv is :  ('小兰', 148)\n",
      "kv is :  ('小红', 89)\n",
      "用zip遍历\n",
      "key: 小明 value:  129\n",
      "type key: <class 'str'> type value: <class 'int'>\n",
      "key: 小兰 value:  148\n",
      "type key: <class 'str'> type value: <class 'int'>\n",
      "key: 小红 value:  89\n",
      "type key: <class 'str'> type value: <class 'int'>\n",
      "用函数读取key和value\n",
      "key: ['小明', '小兰', '小红'] type: <class 'list'>\n",
      "value: [129, 148, 89] type: <class 'list'>\n",
      "字典转列表\n",
      "取key ['a1', 'a2', 'a3']\n",
      "取value ['name1', 'name2', 'name3']\n",
      "列表转字典的方法\n",
      "{0: '有些', 1: '有'}\n"
     ]
    }
   ],
   "source": [
    "dict1 = {\n",
    "    '小明':129,\n",
    "    '小兰':148,\n",
    "    '小红':89\n",
    "}\n",
    "\n",
    "for key in dict1:# 有效\n",
    "    print (key)\n",
    "for value in dict1: # 无效，这样只有key会出来\n",
    "    print(value)\n",
    "for key in dict1.keys():\n",
    "    print(key,type(key))\n",
    "for value in dict1.values():\n",
    "    print(value,type(value))\n",
    "print('同时遍历写法1')\n",
    "for key,value in dict1.items():\n",
    "    print ('key: ',key,'value: ',value)\n",
    "print('同时遍历写法2')\n",
    "for (key,value) in dict1.items():\n",
    "    print ('key: ',key,'value: ',value)\n",
    "print('同时遍历写法3')\n",
    "for kv in dict1.items():\n",
    "    print ('kv is : ',kv)\n",
    "print('用zip遍历')\n",
    "for key,value in zip(dict1.keys(), dict1.values()):\n",
    "    print ('key:',key,'value: ',value)\n",
    "    print('type key:',type(key),'type value:',type(value))\n",
    "\n",
    "print('用函数读取key和value')\n",
    "#定义读key值的函数\n",
    "def keys_function(dict1):\n",
    "    keys = []\n",
    "    #读出key\n",
    "    for k in dict1.keys():\n",
    "        keys.append(format(k))\n",
    "    return keys\n",
    "#定义读出value\n",
    "def values_function(dict1):\n",
    "    values = []\n",
    "    for v in dict1.values():\n",
    "        values.append(v) # format(v)\n",
    "    return values\n",
    "\n",
    "print('key:',keys_function(dict1),'type:',type(keys_function(dict1)))\n",
    "print('value:',values_function(dict1),'type:',type(values_function(dict1)))\n",
    "\n",
    "print('字典转列表')\n",
    "dit = {'a1':'name1',\n",
    "       'a2':'name2',\n",
    "       'a3':'name3'}\n",
    "lst = list(dit.keys())\n",
    "print('取key',lst)\n",
    "lst = list(dit.values())\n",
    "print('取value',lst)\n",
    "\n",
    "\n",
    "print('列表转字典的方法')\n",
    "seg_list = ['有些', '有']\n",
    "seg_index = [0,1]\n",
    "seg = zip(seg_index,seg_list)\n",
    "print(dict(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func(a=xxx)调用函数传入参数名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring mode is on.\n",
      "Exploring mode is on.\n",
      "Exploring mode is on.\n"
     ]
    }
   ],
   "source": [
    "'使用关键字参数'\n",
    "def func(explore=False):\n",
    "    if explore:\n",
    "        print(\"Exploring mode is on.\")\n",
    "\n",
    "func(explore=True)  # 调用时指定 explore 参数为 True\n",
    "\n",
    "'使用字典解包'\n",
    "def func(explore=False):\n",
    "    if explore:\n",
    "        print(\"Exploring mode is on.\")\n",
    "\n",
    "params = {'explore': True}\n",
    "func(**params)  # 使用字典解包来传递参数\n",
    "\n",
    "'使用 **kwargs 来接受任意数量的关键字参数'\n",
    "def func(**kwargs):\n",
    "    if kwargs.get('explore'):\n",
    "        print(\"Exploring mode is on.\")\n",
    "\n",
    "func(explore=True)  # 调用时传递 explore 参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 星号*变量\n",
    "\n",
    "参考来源：https://blog.csdn.net/zkk9527/article/details/88675129\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 最简单的用法是乘，略\n",
    "# 2. *可以收集列表中多余的值\n",
    "a,b,*c=[1,2,3,4]\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [*a,4,5,6]\n",
    "b \n",
    "# ----------------- 输出结果 -----------------\n",
    "# [1, 2, 3, 4, 5, 6]\n",
    "# ----------------- 总结 -----------------\n",
    "# 将a的内容移入（解包）到新列表b中。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在函数定义时,`*`用于收集参数，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "def myprint(*params):\n",
    "    print(params)\n",
    "myprint(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里*将调用时提供的所有值，放在一个元组里\n",
    "\n",
    "跟上面2里的有所区别，2里是收集列表中多余的参数，而这里是收集好参数，一起放进元组里面。\n",
    "\n",
    "这种情况下，在函数定义时的形参里的*params后面，就最好不要再加入别的形参了，比如你定义成 def myprint(*params,x) ，调用的时候myprint（1,2,3），就会报错。因为这样python分不清哪个数据是给params的。如果你非要这么定义也行，不过在调用的时候，必须显示的指出哪个值是给x的。比如myprint（1,2，x=3），通过这种方式调用才不会出错。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在函数定义时,`**`用于收集关键字（key）参数组成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 1, 'y': 2, 'z': 3}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "myprint2() got an unexpected keyword argument 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_508\\2661334299.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmyprint2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmyprint2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#会报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: myprint2() got an unexpected keyword argument 'x'"
     ]
    }
   ],
   "source": [
    "# 正确用法\n",
    "def myprint1(**params):\n",
    "    print(params)\n",
    "myprint1(x=1,y=2,z=3) # 组成字典\n",
    "# 错误用法\n",
    "def myprint2(*params):\n",
    "    print(params)\n",
    "myprint2(x=1,y=2,z=3)  #会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "def funcA(*args):\n",
    "    print(*args)\n",
    "funcA([1,2,3], [4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 函数调用时`*`用于分配参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def myprint(x,y):\n",
    "    print(x)\n",
    "    print(y)\n",
    "params=(1,2)\n",
    "myprint(*params) # 传入的是元组\n",
    "# 只传入一个形参就调用了两个形参的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用函数时，两个`**`的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def myprint(x,y):\n",
    "    print(x)\n",
    "    print(y)\n",
    "params={'x':1,'y':2}\n",
    "myprint(**params) # 传入字典\n",
    "# 只传入一个形参就调用了两个形参的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不要在定义和调用时同时使用`*`号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "技巧：调用函数时可以使用多个`*`变量将多个输入解包并合并为一个，比如\n",
    "```python\n",
    "target_critic_input = torch.cat((*next_states, *target_action), dim=1)\n",
    "```\n",
    "这里的`*next_states`和`*target_action`表示将`next_states`和`target_action`这两个可迭代对象中的元素解包，并将它们作为独立的参数传递给`torch.cat`函数。`torch.cat`函数是PyTorch库中的一个函数，用于连接张量（tensors）。\n",
    "\n",
    "假设`next_states`是一个包含多个张量的列表，`target_action`也是一个张量列表，那么使用`*next_states`和`*target_action`可以将这些张量作为独立的参数传递给`torch.cat`，然后`torch.cat`将这些张量沿着指定的维度`dim=1`进行连接。\n",
    "\n",
    "例如，如果`next_states`是`[A, B, C]`，`target_action`是`[X, Y]`，那么`torch.cat((*next_states, *target_action), dim=1)`等同于`torch.cat([A, B, C, X, Y], dim=1)`，这将把A、B、C、X和Y这些张量在第1维上进行连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他解释：\n",
    "\n",
    "函数定义里面形参变量前面加一个`*`，如`*args`，\n",
    "表示接收任意多个位置参数，储存为元组；\n",
    "变量前有两个`*`的，表示接收任意多个关键字参数，\n",
    "比如 `**kwargs。`\n",
    "单星号变量不仅仅能够用在函数的参数传递中，实际上对一个普通变量使用单星号前缀，能够将这个变量拆分成单个元素。\n",
    "如果在变量前面使用单星号，实际上是对变量的一次拆解操作，将变量中单独的元素拆解出来，然后依次传入one()函数\n",
    "而传入one()函数后，one()函数会将这些传入的单个元素保存成一个元组，这就是为什么我们 print(x[0])能够提取第一个元素的原因\n",
    "变量在传入到单星号变量函数中时，会将变量自动转化为元组，而元组是不能改变的。\n",
    "另外，单星号是无法读取到字典中的值的，永远只会读取到字典中的键（key），如果想读取到字典中的值，需要使用双星号(`**`)\n",
    "双星号可以用来获得字典的值\n",
    "需要注意的是：\n",
    "使用这种方法将字典传入函数的时候，字典的键的命名要符合python变量的命名规则，通过上面的分析也不难看出，双星号会将字典首先转换成关键字参数的形式，就相当于使用字典中的键作为变量名，如果键不符合变量命名规则，则会抛出一个\"TypeError\"异常。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4, 5, 6)\n",
      "{'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6}\n"
     ]
    }
   ],
   "source": [
    "def one(a,*b):\n",
    "    \"\"\"a是一个普通传入参数，*b是一个非关键字星号参数\"\"\"\n",
    "    print(b)\n",
    "one(1,2,3,4,5,6)\n",
    "\n",
    "\n",
    "def two(a=1,**b):\n",
    "    \"\"\"a是一个普通关键字参数，**b是一个关键字双星号参数\"\"\"\n",
    "    print(b)\n",
    "two(a=1,b=2,c=3,d=4,e=5,f=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `*args`和`**kwargs`\n",
    "args 是 arguments 的缩写，表示位置参数；kwargs 是 keyword arguments 的缩写，表示关键字参数。\n",
    "\n",
    "1. ```其实并不是写成 *args 和 **kwargs ，只有前面的 * （星号）才是必须的。```\n",
    "\n",
    "2. ```向python传递参数的方式有两种:位置参数（positional argument）和关键词参数（keyword argument）```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*args 的用法\n",
    "* args 和 ** args 主要用于函数定义，你可以将不定数量的参数传递给一个函数。\n",
    "\n",
    "这里不定的意思是： 预先并不知道，函数使用者会传递多少个参数给你，所在在这个场景下使用这两个关键字。 * args 是用来发送一个 非键值 的可变数量的参数列表给一个函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first normal arg: yasoob\n",
      "another arg through *argv: python\n",
      "another arg through *argv: eggs\n",
      "another arg through *argv: test\n"
     ]
    }
   ],
   "source": [
    "def test_var_args(f_arg, *argv):\n",
    "    print(\"first normal arg:\",f_arg)\n",
    "    for arg in argv:\n",
    "        print(\"another arg through *argv:\",arg)\n",
    "test_var_args('yasoob','python','eggs','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(1, 2, 'python希望社', [])\n"
     ]
    }
   ],
   "source": [
    "def print_func(*args):\n",
    "    print(type(args))\n",
    "    print(args)\n",
    "print_func(1,2,'python希望社',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "1\n",
      "2\n",
      "<class 'tuple'>\n",
      "('python希望社', [])\n"
     ]
    }
   ],
   "source": [
    "def print_func(x,y,*args):\n",
    "    print(type(x))\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(type(args))\n",
    "    print(args)\n",
    "print_func(1,2,'python希望社',[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若 `*args` 不是在最后，则需要在参数传入时，明确定义 `*args`后面的变量参数名(**否则会报错**)，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "x\n",
      "y\n",
      "<class 'tuple'>\n",
      "(1, 2, 'python希望社', [])\n"
     ]
    }
   ],
   "source": [
    "# 改正的代码\n",
    "def print_func(*args,x,y):\n",
    "    print(type(x))\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print(type(args))\n",
    "    print(args)\n",
    "print_func(1,2,'python希望社',[],x='x',y='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** kwargs 的用法\n",
    "\n",
    "```**kwargs允许你将不定长度的 【键值对 key-value 】，作为参数传递给一个函数。如果你想要在一个函数里处理带名字的参数，你应该使用**kwargs```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring mode is on.\n"
     ]
    }
   ],
   "source": [
    "def func(**kwargs):\n",
    "    if kwargs.get('explore'):\n",
    "        print(\"Exploring mode is on.\")\n",
    "\n",
    "func(explore=True)  # 调用时传递 explore 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'a': 1, 'b': 2, 'c': '呵呵哒', 'd': []}\n"
     ]
    }
   ],
   "source": [
    "def print_func(**kwargs):\n",
    "    print(type(kwargs))\n",
    "    print(kwargs)\n",
    "\n",
    "print_func(a=1, b=2, c='呵呵哒', d=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name == yasoob\n"
     ]
    }
   ],
   "source": [
    "def greet_me(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(\"{0} == {1}\".format(key, value))\n",
    "greet_me(name=\"yasoob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(2, 3, 4)\n",
      "{'y': 1, 'a': 2, 'b': 3, 'c': 4}\n"
     ]
    }
   ],
   "source": [
    "'组合使用arg,*arg,**arg'\n",
    "def print_func(x, *args, **kwargs):\n",
    "    print(x)\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "\n",
    "print_func(1, 2, 3, 4, y=1, a=2, b=3, c=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg1: two\n",
      "arg2: 3\n",
      "arg3: 5\n",
      "arg1: 5\n",
      "arg2: two\n",
      "arg3: 3\n"
     ]
    }
   ],
   "source": [
    "def test_args_kwargs(arg1,arg2,arg3):\n",
    "    print(\"arg1:\",arg1)\n",
    "    print(\"arg2:\",arg2)\n",
    "    print(\"arg3:\",arg3)\n",
    "# 首先你可以使用 *args\n",
    "args = (\"two\",3,5)\n",
    "test_args_kwargs(*args)\n",
    "\n",
    "#-------- 得到输出结果如下所示：----------------------\n",
    "# arg1: two\n",
    "# arg2: 3\n",
    "# arg3: 5\n",
    "# ---------------------------------------------------\n",
    "#  现在使用 **kwargs\n",
    "kwargs = {\"arg3\": 3,\"arg2\":\"two\",\"arg1\":5}\n",
    "test_args_kwargs(**kwargs)\n",
    "\n",
    "#-------- 得到输出结果如下所示：----------------------\n",
    "# arg1: 5\n",
    "# arg2: two\n",
    "# arg3: 3\n",
    "# ---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x00000252684D3708>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [4,5,6,7,8]\n",
    "zipped = zip(a,b)\n",
    "print(zipped)\n",
    "list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x000001D108B78488>\n",
      "[(1, 4), (2, 5), (3, 6)]\n",
      "[(1, 7), (2, 8), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "c = [7, 8, 9, 10, 11]\n",
    "zipped01 = zip(a, b)  # 打包为元组的列表\n",
    "zipped02 = zip(a, c)  # 打包为元组的列表\n",
    "print(zipped01)\n",
    "print(list(zipped01))\n",
    "print(list(zipped02))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# 常规解包方法\n",
    "a = (1,2,3) # 元组\n",
    "a1,a2,a3 = (1,2,3)\n",
    "print(a1)\n",
    "print(a2)\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 4), (2, 5), (3, 6), (4, 7), (5, 8)]\n",
      "[(1, 11, 21, 31), (2, 12, 22, 32), (3, 13, 23, 33), (4, 14, 24, 34)]\n",
      "[[1, 11, 21, 31], [2, 12, 22, 32], [3, 13, 23, 33], [4, 14, 24, 34]]\n"
     ]
    }
   ],
   "source": [
    "# 使用zip(*)解包\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# 四种d出来的结果都是一样的，为啥？\n",
    "# d = [[1, 2, 3, 4, 5], [4, 5, 6, 7, 8, 9, 10]]\n",
    "# d = [(1, 2, 3, 4, 5), (4, 5, 6, 7, 8, 9, 10)]\n",
    "# d = ([1, 2, 3, 4, 5], [4, 5, 6, 7, 8, 9, 10])\n",
    "d = ((1, 2, 3, 4, 5), (4, 5, 6, 7, 8, 9, 10))\n",
    "zipped03 = zip(*d)\n",
    "print(list(zipped03)) # 不但解包，而且还转置了\n",
    "# print([zipped03]) 只返回位置\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state',\n",
    "                         'reward'))\n",
    "#生成一个列表，列表中是元组\n",
    "a = [(1,2,3,4),(11,12,13,14),(21,22,23,24),(31,32,33,34)]\n",
    "b = zip(*a)\n",
    "# print(b) 只返回位置\n",
    "b = list(b)\n",
    "c = Transition(*zip(*a))\n",
    "c = list(c)\n",
    "print(c)\n",
    "# 用列表表示转置后的数值\n",
    "print([list(i) for i in zip(*a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 4, 8\n",
      "2, 5, 9\n",
      "3, 6, 10\n"
     ]
    }
   ],
   "source": [
    "# zip(*)会转置而且截断为最短列表长度\n",
    "A = [1, 2, 3]\n",
    "B = [4, 5, 6, 7, 8]\n",
    "C = [8, 9, 10, 11, 12]\n",
    "for a, b, c in zip(A, B, C):\n",
    "    print(f\"{a}, {b}, {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of iters is <class 'zip'>\n",
      "{'a1': 'b1', 'a2': 'b2', 'a3': 'b3'}\n"
     ]
    }
   ],
   "source": [
    "# 声明一个列表\n",
    "nums = [['a1', 'a2', 'a3'], ['b1', 'b2', 'b3']]\n",
    "\n",
    "# 参数为list数组时，是压缩数据，相当于zip()函数\n",
    "iters = zip(*nums)  \n",
    "# 输出zip(*zipped)函数返回对象的类型\n",
    "print(\"type of iters is %s\" % type(iters))  \n",
    "# 因为zip(*zipped)函数返回一个zip类型对象，所以我们需要对其进行转换\n",
    "# 在这里，我们将其转换为字典\n",
    "print(dict(iters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：该dict()函数可用于将zip对象转换为字典。需要注意的是，只能使用两个zip()参数，前者产生key，后者产生value。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *zip()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*zip(m, n)返回: (1, 4) (2, 5) (3, 6)\n",
      "m2和n2的值分别为: (1, 2, 3) (4, 5, 6)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 创建2个列表\n",
    "m = [1, 2, 3]\n",
    "n = [4, 5, 6]\n",
    "\n",
    "print(\"*zip(m, n)返回:\", *zip(m, n))\n",
    "m2, n2 = zip(*zip(m, n))\n",
    "print(\"m2和n2的值分别为:\", m2, n2)\n",
    "# 若相等，返回True；说明*zip为zip的逆过程\n",
    "print(m == list(m2) and n == list(n2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(name='kongxx', sex='male', age=21)\n",
      "kongxx\n",
      "male\n",
      "21\n",
      "User(name='kongxx', sex='male', age=22)\n",
      "OrderedDict([('name', 'kongxx'), ('sex', 'male'), ('age', 22)])\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# 定义一个namedtuple类型User，并包含name，sex和age属性。\n",
    "User = namedtuple('User', ['name', 'sex', 'age'])\n",
    "\n",
    "# 创建一个User对象\n",
    "user = User(name='kongxx', sex='male', age=21)\n",
    "\n",
    "# 也可以通过一个list来创建一个User对象，这里注意需要使用\"_make\"方法\n",
    "user = User._make(['kongxx', 'male', 21])\n",
    "\n",
    "print(user)\n",
    "# User(name='user1', sex='male', age=21)\n",
    "\n",
    "# 获取用户的属性\n",
    "print(user.name)\n",
    "print(user.sex)\n",
    "print(user.age)\n",
    "\n",
    "# 修改对象属性，注意要使用\"_replace\"方法\n",
    "user = user._replace(age=22)\n",
    "print(user)\n",
    "# User(name='user1', sex='male', age=21)\n",
    "\n",
    "# 将User对象转换成字典，注意要使用\"_asdict\"\n",
    "print(user._asdict())\n",
    "# OrderedDict([('name', 'kongxx'), ('sex', 'male'), ('age', 22)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出一个与shape参数指定大小的矩阵，矩阵的数值正态分布采样生成，并且这个3*4的矩阵的数值的上限为high参数，下限为low参数。\n",
      "\r",
      " [[-0.82098436  0.16519162  1.656999    0.6244945 ]\n",
      " [ 0.8260099   1.2651362  -0.6286092   1.3861233 ]\n",
      " [-0.7776112   1.6446414   0.4964102   0.2741933 ]] \n",
      "不对数据sample\n",
      "\r",
      " Box(-1.0, 2.0, (3, 4), float32) \n",
      "输出一个1 * 2（一行两列）的ndarray类型（ndarray类型是numpy包里定义的数据类型）的数组，数组的数值正态分布采样生成，并且这个1*2的数组的数值的上限为high参数，下限为low参数。\n",
      "[ 1.4676481 -0.7341325]\n",
      "[1.1444509 1.4857175]\n",
      "[0.5865058  0.17672935]\n",
      "[-0.33631134 -0.16783987]\n",
      "[ 1.0294192 -1.4618783]\n",
      "[1.3939005 0.816857 ]\n",
      "[1.7860607 1.8121102]\n",
      "[1.6478595 1.725681 ]\n",
      "[1.6674246 3.0977757]\n",
      "[-0.46322843 -0.56891364]\n",
      "\n",
      "不对数据sample\n",
      "Box([-1. -2.], [2. 4.], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "from gym import spaces\n",
    "import numpy as np\n",
    "print('输出一个与shape参数指定大小的矩阵，矩阵的数值正态分布采样生成，并且这个3*4的矩阵的数值的上限为high参数，下限为low参数。')\n",
    "a=spaces.Box(low=-1,high=2,shape=(3,4))\n",
    "print(\"\\r\",a.sample(),end=\" \")\n",
    "print('\\n不对数据sample')\n",
    "a=spaces.Box(low=-1,high=2,shape=(3,4))\n",
    "print(\"\\r\",a,end=\" \")\n",
    "print('\\n输出一个1 * 2（一行两列）的ndarray类型（ndarray类型是numpy包里定义的数据类型）的数组，数组的数值正态分布采样生成，并且这个1*2的数组的数值的上限为high参数，下限为low参数。')\n",
    "a=spaces.Box(low=np.array([-1, -2]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
    "for i in range(10):\n",
    "    print(a.sample())\n",
    "print('\\n不对数据sample')\n",
    "a=spaces.Box(low=np.array([-1, -2]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random.choice用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[4 1 1]\n",
      "[2 2 2 5 5]\n",
      "[6 6 2 2 6]\n",
      "[2 2 4 1 4]\n",
      "[0 4 2 1 1 3]\n",
      "[4 2 1 0 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "'从0到n抽取'\n",
    "print(np.random.choice(5))#从[0, 5)中随机输出一个随机数\n",
    "print(np.random.choice(5, 3))#在[0, 5)内输出3个数字并组成一维数组（ndarray）\n",
    "'从数组、列表或元组中抽取'\n",
    "L = [1, 2, 3, 4, 5]#list列表\n",
    "T = (2, 4, 6, 2)#tuple元组\n",
    "A = np.array([4, 2, 1])#numpy,array数组,必须是一维的\n",
    "#二维数组会报错，必须用一维数组输入\n",
    "print(np.random.choice(L, 5))\n",
    "print(np.random.choice(T, 5))\n",
    "print(np.random.choice(A, 5))\n",
    "'参数是否可取相同元素'\n",
    "print(np.random.choice(5, 6, replace=True))#可以看到有相同元素\n",
    "print(np.random.choice(5, 5, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转换维度\n",
      "torch.Size([4, 4])\n",
      "torch.Size([16])\n",
      "torch.Size([2, 8])\n",
      "torch.Size([2, 2, 4])\n",
      "转换为一维\n",
      "tensor([1., 2., 3., 4., 5., 6.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print('转换维度')\n",
    "x = torch.randn(4, 4)\n",
    "print(x.size())\n",
    "y = x.view(16)\n",
    "print(y.size())\n",
    "z = x.view(-1, 8)  # -1表示该维度取决于其它维度大小，即（4*4）/ 8\n",
    "print(z.size())\n",
    "m = x.view(2, 2, 4) # 也可以变为更多维度\n",
    "print(m.size())\n",
    "print('转换为一维')\n",
    "a = torch.Tensor([[1, 2, 3], [4, 5, 6]]) # 定义一个 2*3 的 Tensor\n",
    "a = a.view(-1)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3308, 0.5350],\n",
       "        [0.0354, 1.0000],\n",
       "        [1.5321, 1.0000]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.randn(3,2)\n",
    "y=torch.ones(3,2)\n",
    "torch.where(x>0,x,y) # 条件，是就怎样，不是又怎样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 7, 1, 5],\n",
      "        [7, 8, 8, 7]])\n",
      "torch.return_types.max(\n",
      "values=tensor([7, 8]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "# (max, max_indices) = torch.max(input, dim, keepdim=False)\n",
    "# 输入：\n",
    "# input 是输入的tensor,dim 是索引的维度，\n",
    "# dim=0寻找每一列的最大值，dim=1寻找每一行的最大值,\n",
    "# keepdim 表示是否需要保持输出的维度与输入一样，\n",
    "# keepdim=True表示输出和输入的维度一样，\n",
    "# keepdim=False表示输出的维度被压缩了，\n",
    "# 也就是输出会比输入低一个维度。\n",
    "# 输出：\n",
    "# max 表示取最大值后的结果，max_indices 表示最大值的索引。\n",
    "\n",
    "x = torch.randint(0,9,(2,4))\n",
    "print(x)\n",
    "y = torch.max(x, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7],\n",
      "        [8]])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "#取每一行的最大值，torch.max的输出结果\n",
    "y = torch.max(x, 1, keepdim=True)[0]\n",
    "# keepdim=True，输出仍然是二维的\n",
    "print(y)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "y = torch.max(x, 1, keepdim=False)[0]\n",
    "# keepdim=False，输出变成了一维\n",
    "print(y)\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .detach() 和.detach_()（后者少用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***tensor.detach()*** 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。即使之后重新将它的requires_grad置为true,它也不会具有梯度grad，这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的tensor就会停止，不能再继续向前进行传播。\n",
    "注意：\n",
    "使用detach返回的tensor和原始的tensor共同一个内存，即一个修改另一个也会跟着改变。\n",
    "\n",
    "***tensor.detach_()*** 将一个tensor从创建它的图中分离，并把它设置成叶子tensor，其实就相当于变量之间的关系本来是x -> m -> y,这里的叶子tensor是x，但是这个时候对m进行了m.detach_()操作,其实就是进行了两个操作：\n",
    "\n",
    "1、将m的grad_fn的值设置为None,这样m就不会再与前一个节点x关联，这里的关系就会变成x, m -> y,此时的m就变成了叶子结点\n",
    "\n",
    "2、然后会将m的requires_grad设置为False，这样对y进行backward()时就不会求m的梯度\n",
    "\n",
    "总结：其实detach()和detach_()很像，两个的区别就是detach_()是对本身的更改，detach()则是生成了一个新的tensor，比如x -> m -> y中如果对m进行detach()，后面如果反悔想还是对原来的计算图进行操作还是可以的，但是如果是进行了detach_()，那么原来的计算图也发生了变化，就不能反悔了.\n",
    "\n",
    "来源：https://blog.csdn.net/qq_43722079/article/details/136583592\n",
    "https://blog.csdn.net/qq_27825451/article/details/95498211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0.1966, 0.1050, 0.0452])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "print(a.grad)\n",
    "out = a.sigmoid()\n",
    "out.sum().backward()# 求导常见写法，等同于out.backward(gradient=torch.ones(len(out)))\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "tensor([0.1966, 0.1050, 0.0452])\n"
     ]
    }
   ],
   "source": [
    "'当使用detach()分离tensor但是没有更改这个tensor时，'\n",
    "'并不会影响backward():'\n",
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "print(a.grad)\n",
    "out = a.sigmoid()\n",
    "print(out)\n",
    "\n",
    "#添加detach(),c的requires_grad为False\n",
    "c = out.detach()\n",
    "print(c)\n",
    "\n",
    "#这时候没有对c进行更改，所以并不会影响backward()\n",
    "out.sum().backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41604\\4276394133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#使用新生成的Variable进行反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "'当使用detach()分离tensor，然后用这个分离出来的tensor去求导数，'\n",
    "'会影响backward()，会出现错误'\n",
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "print(a.grad)\n",
    "out = a.sigmoid()\n",
    "print(out)\n",
    "\n",
    "#添加detach(),c的requires_grad为False\n",
    "c = out.detach()\n",
    "print(c)\n",
    "\n",
    "#使用新生成的Variable进行反向传播\n",
    "c.sum().backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([0.7311, 0.8808, 0.9526], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.7311, 0.8808, 0.9526])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0.], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_41604\\994537329.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#这时候对c进行更改，所以会影响backward()，这时候就不能进行backward()，会报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\37\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniforge3\\envs\\37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [3]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "'当使用detach()分离tensor并且更改这个tensor时，'\n",
    "'即使再对原来的out求导数，会影响backward()，会出现错误'\n",
    "a = torch.tensor([1, 2, 3.], requires_grad=True)\n",
    "print(a.grad)\n",
    "out = a.sigmoid()\n",
    "print(out)\n",
    "\n",
    "#添加detach(),c的requires_grad为False\n",
    "c = out.detach()\n",
    "print(c)\n",
    "c.zero_() #使用in place函数对其进行修改\n",
    "\n",
    "#会发现c的修改同时会影响out的值\n",
    "print(c)\n",
    "print(out)\n",
    "\n",
    "#这时候对c进行更改，所以会影响backward()，这时候就不能进行backward()，会报错\n",
    "out.sum().backward()\n",
    "print(a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## @用法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def decorator(func):\n",
    "   return func\n",
    " \n",
    "@decorator\n",
    "def some_func():\n",
    "    pass\n",
    "```\n",
    "等效于以下代码：\n",
    "```python\n",
    "def decorator(func):\n",
    "    return func\n",
    " \n",
    "def some_func():\n",
    "    pass\n",
    " \n",
    "some_func = decorator(some_func)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheese', 'sauce']\n"
     ]
    }
   ],
   "source": [
    "class Pizza(object):\n",
    "    def __init__(self):\n",
    "        self.toppings = []\n",
    " \n",
    "    def __call__(self, topping):\n",
    "        # When using '@instance_of_pizza' before a function definition\n",
    "        # the function gets passed onto 'topping'.\n",
    "        self.toppings.append(topping())\n",
    " \n",
    "    def __repr__(self):\n",
    "        return str(self.toppings)\n",
    " \n",
    "pizza = Pizza()\n",
    "@pizza\n",
    "def cheese():\n",
    "    return 'cheese'\n",
    "@pizza\n",
    "def sauce():\n",
    "    return 'sauce'\n",
    "print(pizza)\n",
    "# ['cheese', 'sauce']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这表明在修饰符之后定义的function / method / class基本上只是在@符号之后作为argument传递给function / method。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下两种写法等同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithoutDecorators:\n",
    "    def some_static_method():\n",
    "        print(\"this is static method\")\n",
    "    some_static_method = staticmethod(some_static_method)\n",
    "\n",
    "    def some_class_method(cls):\n",
    "        print(\"this is class method\")\n",
    "    some_class_method = classmethod(some_class_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WithDecorators:\n",
    "    @staticmethod\n",
    "    def some_static_method():\n",
    "        print(\"this is static method\")\n",
    " \n",
    "    @classmethod\n",
    "    def some_class_method(cls):\n",
    "        print(\"this is class method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常用的@classmethod、@staticmethod和@property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @classmethod：\n",
    "\n",
    "- 这是一个装饰器，它将一个方法定义为类方法。\n",
    "- 类方法的第一个参数是类本身，通常使用cls作为参数名。\n",
    "- 可以通过类直接调用类方法，也可以通过类的实例调用。\n",
    "- 类方法可以访问和修改类属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MyClass'>\n",
      "<class '__main__.MyClass'>\n",
      "hello1\n",
      "hello2\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def hello1():\n",
    "        print('hello1')\n",
    "    def hello2(self):\n",
    "        print('hello2')\n",
    "    @classmethod\n",
    "    def my_method(cls):\n",
    "        print(cls)\n",
    "        \n",
    "instance = MyClass() # 实例化\n",
    "MyClass.my_method()  # 通过类调用 \n",
    "instance.my_method()  # 通过实例调用\n",
    "MyClass.hello1() # 可以通过类调用普通方法\n",
    "instance.hello2() # 实例调用不了hello\n",
    "# hello1无法用实例调用，hello2无法通过类本身调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Date(object):\n",
    "    \n",
    "    def __init__(self, day=0, month=0, year=0):\n",
    "        self.day = day\n",
    "        self.month = month\n",
    "        self.year = year\n",
    "\n",
    "    @classmethod\n",
    "#   cls 代表的是类本身，而不是类的实例。这相当酷，因为如果我们继承了我们的 Date 类，所有的子类也都会有 from_string 方法。\n",
    "    def from_string(cls, date_as_string):\n",
    "        day, month, year = map(int, date_as_string.split('-'))\n",
    "        date1 = cls(day, month, year)\n",
    "        return date1\n",
    "\n",
    "    @staticmethod\n",
    "    def is_date_valid(date_as_string):\n",
    "        day, month, year = map(int, date_as_string.split('-'))\n",
    "        return day <= 31 and month <= 12 and year <= 3999\n",
    "\n",
    "date2 = Date.from_string('11-09-2012')\n",
    "is_date = Date.is_date_valid('11-09-2012')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @staticmethod\n",
    "- 这是一个装饰器，它将一个方法定义为静态方法。\n",
    "- 静态方法不需要类或实例的引用作为第一个参数。\n",
    "- 静态方法不能访问类的状态（即类属性或实例属性）。\n",
    "- 它们通常用于实现与类相关但不需要类或实例数据的功能。\n",
    "\n",
    "静态方法与实例方法之间有几个重要的区别：\n",
    "- 参数传递：静态方法不需要接收类或实例作为第一个参数，因此不需要 self 或 cls 参数。\n",
    "- 访问属性：静态方法不能直接访问类或实例的属性，因为它们不接收 self 或 cls 参数。\n",
    "- 调用方式：静态方法可以通过类名直接调用，而实例方法需要通过类的实例调用。\n",
    "\n",
    "辅助函数\n",
    "- 静态方法常用于定义一些与类密切相关但不需要访问实例属性的辅助函数。这些函数通常用于执行特定的任务或提供特定的功能，与类的其他方法共同完成某个操作。\n",
    "```python\n",
    "class MathUtil:\n",
    "    @staticmethod\n",
    "    def add(x, y):\n",
    "        return x + y\n",
    " \n",
    "    @staticmethod\n",
    "    def subtract(x, y):\n",
    "        return x - y\n",
    "# 调用静态方法\n",
    "print(MathUtil.add(5, 3))        # 输出: 8\n",
    "print(MathUtil.subtract(5, 3))   # 输出: 2\n",
    "```\n",
    "\n",
    "类功能相关的函数组\n",
    "- 有时候，可能需要在一个类中定义一组功能相关的函数，这些函数共同完成某个任务，但不需要访问实例的状态。静态方法可以很好地满足这种需求，使得代码更加模块化和可维护。\n",
    "\n",
    "```python\n",
    "class FileUtils:\n",
    "    @staticmethod\n",
    "    def get_file_extension(filename):\n",
    "        return filename.split('.')[-1]\n",
    " \n",
    "    @staticmethod\n",
    "    def is_image(filename):\n",
    "        extensions = ['jpg', 'jpeg', 'png', 'gif']\n",
    "        return FileUtils.get_file_extension(filename).lower() in extensions\n",
    "# 使用静态方法检查文件是否为图片\n",
    "print(FileUtils.is_image('example.jpg'))   # 输出: True\n",
    "print(FileUtils.is_image('document.pdf'))  # 输出: False\n",
    "```\n",
    "工厂函数\n",
    "- 静态方法常常被用作工厂函数，用于创建类的实例。工厂函数在创建实例时提供了更灵活的方式，可以根据传入的参数不同返回不同类型的实例。\n",
    "```python\n",
    "class Shape:\n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    " \n",
    "    @staticmethod\n",
    "    def create_rectangle(width, height):\n",
    "        return Shape(width, height)\n",
    " \n",
    "    @staticmethod\n",
    "    def create_square(side_length):\n",
    "        return Shape(side_length, side_length)\n",
    "\n",
    "# 使用静态方法创建不同类型的形状实例\n",
    "rectangle = Shape.create_rectangle(4, 6)\n",
    "square = Shape.create_square(5)\n",
    " \n",
    "print(rectangle.width, rectangle.height)  # 输出: 4 6\n",
    "print(square.width, square.height)        # 输出: 5 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a static method.\n",
      "This is a static method.\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    @staticmethod\n",
    "    def my_method():\n",
    "        print(\"This is a static method.\")\n",
    "\n",
    "MyClass.my_method()  # 静态方法可通过类调用\n",
    "instance = MyClass()\n",
    "instance.my_method()  # 也可以通过实例调用，但实例参数会被忽略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @property \n",
    "- @property是 Python 中的一个装饰器，它用于将一个方法转变为属性。使用 @property 可以创建只读属性，或者在属性被访问时执行特定的代码。\n",
    "- 当你使用 @property 装饰器时，你实际上是在定义一个 getter 方法，该方法可以被像普通属性一样访问。此外，你还可以使用 @<property_name>.setter 装饰器来定义一个 setter 方法，用于设置属性的值，并在设置值时执行特定的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中：\n",
    "- @property 装饰器定义了 name 属性的 getter 方法。\n",
    "- @name.setter 装饰器定义了 name 属性的 setter 方法，它还包含了一些逻辑来确保设置的名称是字符串类型。\n",
    "\n",
    "使用 @property 的好处包括：\n",
    "- 提供对属性访问的控制，可以在访问或设置属性时添加逻辑。\n",
    "- 代码的封装性更好，隐藏了属性背后的实现细节。\n",
    "- 使得类的接口看起来更像一个简单的属性访问，而不是方法调用。\n",
    "- 使用 @property 是一种实现封装和数据验证的好方法，同时保持了代码的简洁性和易用性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n",
      "Bob\n"
     ]
    }
   ],
   "source": [
    "class Person:\n",
    "    def __init__(self, name):\n",
    "        self._name = name  # 使用一个下划线前缀来表示这是一个受保护的属性\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @name.setter\n",
    "    def name(self, value):\n",
    "        if not isinstance(value, str):\n",
    "            raise ValueError(\"Name must be a string.\")\n",
    "        self._name = value\n",
    "\n",
    "# 使用\n",
    "person = Person(\"Alice\")\n",
    "print(person.name)  # 使用@property访问属性\n",
    "\n",
    "person.name = \"Bob\"  # 使用@name.setter设置属性\n",
    "print(person.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. property 应用场景\n",
    "- 在获取、设置和删除对象属性的时候，需要额外做一些工作。比如在游戏编程中，设置敌人死亡之后需要播放死亡动画。\n",
    "- 需要限制对象属性的设置和获取。比如用户年龄为只读，或者在设置用户年龄的时候有范围限制。\n",
    "- 这时就可以使用 property 工具，它把方法包装成属性，让方法可以以属性的形式被访问和调用。\n",
    "2. property() 函数\n",
    "- 语法：property(fget=None, fset=None, fdel=None, doc=None) -> property attribute\n",
    "- 说明：\n",
    "    fget 是获取属性值的方法。\n",
    "    fset 是设置属性值的方法。\n",
    "    fdel 是删除属性值的方法。\n",
    "- doc 是属性描述信息。如果省略，会把 fget 方法的 docstring 拿来用（如果有的话）\n",
    "示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查看属性的文档字符串：学生年龄\n",
      "设置属性时执行的代码\n",
      "获取属性时执行的代码\n",
      "学生年龄为：18\n",
      "删除属性时执行的代码\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n删除属性时执行的代码\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Student:\n",
    "    def __init__(self):\n",
    "        self._age = None\n",
    "\n",
    "    def get_age(self):\n",
    "        print('获取属性时执行的代码')\n",
    "        return self._age\n",
    "\n",
    "    def set_age(self, age):\n",
    "        print('设置属性时执行的代码')\n",
    "        self._age = age\n",
    "\n",
    "    def del_age(self):\n",
    "        print('删除属性时执行的代码')\n",
    "        del self._age\n",
    "\n",
    "    age = property(get_age, set_age, del_age, '学生年龄')\n",
    "\n",
    "\n",
    "student = Student()\n",
    "# 注意要用 类名.属性.__doc__ 的形式查看属性的文档字符串\n",
    "print('查看属性的文档字符串：' + Student.age.__doc__)\n",
    "\"\"\"\n",
    "查看属性的文档字符串：学生年龄\n",
    "\"\"\"\n",
    "\n",
    "# 设置属性\n",
    "student.age = 18\n",
    "\"\"\"\n",
    "设置属性时执行的代码\n",
    "\"\"\"\n",
    "\n",
    "# 获取属性\n",
    "print('学生年龄为：' + str(student.age))\n",
    "\"\"\"\n",
    "获取属性时执行的代码\n",
    "学生年龄为：18\n",
    "\"\"\"\n",
    "\n",
    "# 删除属性\n",
    "del student.age\n",
    "\"\"\"\n",
    "删除属性时执行的代码\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. @property 装饰器\n",
    "@property 语法糖提供了比 property() 函数更简洁直观的写法。\n",
    "- 被 @property 装饰的方法是获取属性值的方法，被装饰方法的名字会被用做 属性名。\n",
    "- 被 @属性名.setter 装饰的方法是设置属性值的方法。\n",
    "- 被 @属性名.deleter 装饰的方法是删除属性值的方法。\n",
    "以下示例代码与使用 property() 函数版本的代码等价："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "设置属性时执行的代码\n",
      "获取属性时执行的代码\n",
      "学生年龄为：18\n",
      "删除属性时执行的代码\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n删除属性时执行的代码\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Student:\n",
    "    def __init__(self):\n",
    "        self._age = None\n",
    "\n",
    "    @property\n",
    "    def age(self):\n",
    "        print('获取属性时执行的代码')\n",
    "        return self._age\n",
    "\n",
    "    @age.setter\n",
    "    def age(self, age):\n",
    "        print('设置属性时执行的代码')\n",
    "        self._age = age\n",
    "\n",
    "    @age.deleter\n",
    "    def age(self):\n",
    "        print('删除属性时执行的代码')\n",
    "        del self._age\n",
    "\n",
    "\n",
    "student = Student()\n",
    "\n",
    "# 设置属性\n",
    "student.age = 18\n",
    "\"\"\"\n",
    "设置属性时执行的代码\n",
    "\"\"\"\n",
    "\n",
    "# 获取属性\n",
    "print('学生年龄为：' + str(student.age))\n",
    "\"\"\"\n",
    "获取属性时执行的代码\n",
    "学生年龄为：18\n",
    "\"\"\"\n",
    "\n",
    "# 删除属性\n",
    "del student.age\n",
    "\"\"\"\n",
    "删除属性时执行的代码\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Python中，@property 装饰器用于将一个方法变为属性访问器，使得该方法可以通过点操作符（.）访问，就像访问普通的属性一样。当你使用 @property 装饰器时，通常会配合一个 getter 方法，有时还包括 setter 方法和 deleter 方法。\n",
    "\n",
    "下面是 @property 装饰器的基本用法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, value):\n",
    "        self._my_attribute = value\n",
    "\n",
    "    @property\n",
    "    def my_attribute(self):\n",
    "        # getter 方法\n",
    "        return self._my_attribute\n",
    "\n",
    "    @my_attribute.setter\n",
    "    def my_attribute(self, value):\n",
    "        # setter 方法\n",
    "        self._my_attribute = value\n",
    "\n",
    "    @my_attribute.deleter\n",
    "    def my_attribute(self):\n",
    "        # deleter 方法\n",
    "        del self._my_attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中：\n",
    "\n",
    "- @property 装饰器应用于 my_attribute 方法，将其转换为属性访问器。这意味着你可以通过 obj.my_attribute 来访问这个属性。\n",
    "- @my_attribute.setter 装饰器用于定义一个设置器（setter），允许你为属性赋值。设置器的第一个参数通常是 value，表示要设置的新值。\n",
    "- @my_attribute.deleter 装饰器用于定义一个删除器（deleter），允许你删除这个属性。\n",
    "只有被 @property 装饰的方法才会被视为属性访问器，并且它下面的直接方法定义（通常是 setter 方法）会与它关联。如果你在 @property 下面定义了多个方法，只有紧跟在 @property 后面的第一个方法会被当作 getter 方法，而其他的方法则不会被当作属性的一部分。\n",
    "\n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    def __init__(self, value):\n",
    "        self._my_attribute = value\n",
    "\n",
    "    @property\n",
    "    def my_attribute(self):\n",
    "        # 这是 getter 方法\n",
    "        return self._my_attribute\n",
    "\n",
    "    def some_other_method(self):\n",
    "        # 这个方法不受 @property 的影响\n",
    "        pass\n",
    "\n",
    "    @my_attribute.setter\n",
    "    def my_attribute(self, value):\n",
    "        # 这是 setter 方法，与 @property 相关联\n",
    "        self._my_attribute = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个例子中，some_other_method 不受 @property 的影响，因为它不是 my_attribute 属性的一部分。只有紧跟在 @property 装饰器后面的 my_attribute 方法和 @my_attribute.setter 装饰的 my_attribute 方法会构成属性的 getter 和 setter。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类和结构体\n",
    "python中没有结构体，但是可以用类、命名元祖和字典代替"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）用类代替结构体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.data: 233333\n",
      "a.datalen: 6\n",
      "a.datatype: <class 'str'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 首先定义一个类，要有__init__\n",
    "class SN:\n",
    "    def __init__(self):\n",
    "        self.data = \"\"\n",
    "        self.datalen = \"\"\n",
    "        self.datatype = \"\"\n",
    "\n",
    "# 开始初始化结构体\n",
    "a = SN()\n",
    "a.data = \"233333\"\n",
    "a.datalen = len(a.data)\n",
    "a.datatype = type(a.data)\n",
    "\n",
    "print(\n",
    "    f\"a.data: {a.data}\\n\"\n",
    "    f\"a.datalen: {a.datalen}\\n\"\n",
    "    f\"a.datatype: {a.datatype}\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class test:\n",
    "    def __init__(self):\n",
    "        self.a=1\n",
    "        self.b=2\n",
    "Test=test()\n",
    "print(Test.a)\n",
    "Test.b=1\n",
    "print(Test.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（2）用命名元组代替结构体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 1\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "MyStruct = namedtuple(\"MyStruct\", [\"var1\", \"var2\", \"var3\"])\n",
    "\n",
    "# 创建结构体实例\n",
    "my_struct = MyStruct(\"Value 1\", 2, True)\n",
    "\n",
    "# 访问结构体成员变量\n",
    "print(my_struct.var1)  # 输出：Value 1\n",
    "print(my_struct.var2)  # 输出：2\n",
    "print(my_struct.var3)  # 输出：True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）用字典代替结构体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 1\n",
      "2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "my_struct = {\n",
    "    \"var1\": \"Value 1\",\n",
    "    \"var2\": 2,\n",
    "    \"var3\": True\n",
    "}\n",
    "\n",
    "# 访问结构体成员变量\n",
    "print(my_struct[\"var1\"])  # 输出：Value 1\n",
    "print(my_struct[\"var2\"])  # 输出：2\n",
    "print(my_struct[\"var3\"])  # 输出：True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（4）用argparse传入数值（后面有写）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承、抽象与组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 继承\n",
    " 假如已经有几个类，而类与类之间有共同的变量属性和函数属性，那就可以把这几个变量属性和函数属性提取出来作为基类的属性。而特殊的变量属性和函数属性，则在本类中定义，这样只需要继承这个基类，就可以访问基类的变量属性和函数属性。可以提高代码的可扩展性。\n",
    " \n",
    " 使用继承的话,任何一点小的变化也需要重新定义一个类,很容易引起类的爆炸式增长,产生一大堆有着细微不同的子类. 所以有个 **“多用组合少用继承”** 的原则。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof!\n",
      "Meow!\n"
     ]
    }
   ],
   "source": [
    "# 在Python中，类的继承是通过在类定义时在括号中指定一个或多个父类（基类或超类）来实现的。子类（派生类）会继承父类的所有属性和方法。如果子类需要扩展或修改父类的行为，可以在子类中添加或重写相应的方法。\n",
    "# 下面是一个简单的Python类继承的例子：\n",
    "\n",
    "# 定义一个父类\n",
    "class Animal:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def speak(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "# 定义一个子类，继承自Animal\n",
    "class Dog(Animal):\n",
    "    def speak(self):\n",
    "        return \"Woof!\"\n",
    "\n",
    "# 定义另一个子类，同样继承自Animal\n",
    "class Cat(Animal):\n",
    "    def speak(self):\n",
    "        return \"Meow!\"\n",
    "\n",
    "# 创建子类的实例\n",
    "dog = Dog(\"Buddy\")\n",
    "cat = Cat(\"Whiskers\")\n",
    "\n",
    "# 调用子类的方法\n",
    "print(dog.speak())  # 输出: Woof!\n",
    "print(cat.speak())  # 输出: Meow!\n",
    "\n",
    "# 在这个例子中：\n",
    "# - `Animal` 类是一个父类，它有一个初始化方法 `__init__` 和一个抽象方法 `speak`。\n",
    "# - `Dog` 和 `Cat` 类是子类，它们都继承自 `Animal` 类。\n",
    "# - 子类 `Dog` 和 `Cat` 都重写了 `speak` 方法，以提供具体的实现。\n",
    "\n",
    "# Python支持多重继承，这意味着一个子类可以同时继承多个父类。例如：\n",
    "class A:\n",
    "    pass\n",
    "\n",
    "class B:\n",
    "    pass\n",
    "\n",
    "class C(A, B):\n",
    "    pass\n",
    "\n",
    "# 在这个例子中，类 `C` 继承自两个父类 `A` 和 `B`。\n",
    "# 继承是面向对象编程的一个重要特性，它允许代码复用，同时提供了一种组织和管理大型软件项目中复杂性的方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 抽象（先抽象再继承）\n",
    "抽象即提取类似的部分。\n",
    "基类就是抽象多个类共同的属性得到的一个类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人在塔在\n",
      "356\n"
     ]
    }
   ],
   "source": [
    "class Riven:\n",
    "    camp='Noxus'\n",
    "    def __init__(self,nickname,\n",
    "                 script,\n",
    "                 aggressivity=54,\n",
    "                 life_value=414,\n",
    "                 ):\n",
    " \n",
    "        self.nickname = nickname\n",
    "        self.aggressivity = aggressivity\n",
    "        self.life_value = life_value\n",
    "        self.script=script\n",
    " \n",
    "    def attack(self,enemy):\n",
    "        print(self.script)\n",
    "        enemy.life_value -= self.aggressivity\n",
    " \n",
    " \n",
    "class Garen:\n",
    "    camp='Demacia'\n",
    "    def __init__(self,nickname,\n",
    "                 script,\n",
    "                 aggressivity=58,\n",
    "                 life_value=455,\n",
    "                 ):\n",
    "        self.nickname = nickname\n",
    "        self.aggressivity = aggressivity\n",
    "        self.life_value = life_value\n",
    "        self.script = script\n",
    " \n",
    "    def attack(self,enemy):\n",
    "        print(self.script)\n",
    "        enemy.life_value -= self.aggressivity\n",
    " \n",
    " \n",
    "g1=Garen(\"德玛西亚之力\",\"人在塔在\")\n",
    "g1.camp=\"诺克萨斯\"\n",
    "r1=Riven(\"瑞雯\",\"断剑重铸之日，骑士归来之时\")\n",
    "g1.attack(r1)\n",
    "print(r1.life_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "严格来说，上述Hero.init(self,...)，不能算作子类调用父类的方法。因为我们如果去掉（Hero）这个继承关系，代码仍能得到预期的结果。\n",
    "总结python中继承的特点：\n",
    "1. 在子类中，并不会自动调用基类的__init__()，需要在派生类中手动调用。\n",
    "2. 在调用基类的方法时，需要加上基类的类名前缀，且需要带上self参数变量。\n",
    "3. 先在本类中查找调用的方法，找不到才去基类中找。\n",
    "\n",
    "### 组合\n",
    "代码复用的重要的方式除了继承，还有组合。\n",
    "组合，在一个类中以另一个类的对象作为数据属性，称为类的组合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release Fire skill\n"
     ]
    }
   ],
   "source": [
    "# 常规写法1（在类方法中访问）\n",
    "class Skill:\n",
    "    def fire(self):\n",
    "        print(\"release Fire skill\")\n",
    "\n",
    "class Riven:\n",
    "    camp='Noxus'\n",
    "    def __init__(self,nickname):\n",
    "        self.nickname=nickname\n",
    "        self.skill5=Skill().fire()#Skill类产生一个对象，并调用fire()方法,赋值给实例的skill5属性\n",
    "\n",
    "r1=Riven(\"瑞雯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine is starting.\n",
      "Wheel is rotating.\n",
      "Wheel is rotating.\n",
      "Wheel is rotating.\n",
      "Wheel is rotating.\n",
      "Car is driving.\n"
     ]
    }
   ],
   "source": [
    "# 常规写法2（在类初始化时访问）\n",
    "class Engine:\n",
    "    def start(self):\n",
    "        print(\"Engine is starting.\")\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"Engine is stopping.\")\n",
    "\n",
    "class Wheel:\n",
    "    def rotate(self):\n",
    "        print(\"Wheel is rotating.\")\n",
    "\n",
    "class Car:\n",
    "    def __init__(self):\n",
    "        self.engine = Engine()\n",
    "        self.wheels = [Wheel() for _ in range(4)]\n",
    "\n",
    "    def start_engine(self):\n",
    "        self.engine.start()\n",
    "\n",
    "    def stop_engine(self):\n",
    "        self.engine.stop()\n",
    "\n",
    "    def drive(self):\n",
    "        self.start_engine()\n",
    "        for wheel in self.wheels:\n",
    "            wheel.rotate()\n",
    "        print(\"Car is driving.\")\n",
    "\n",
    "# 使用组合\n",
    "my_car = Car()\n",
    "my_car.drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继承的方式\n",
    "通过继承建立了派生类与基类之间的关系，它是一种'是'的关系，比如白马是马，人是动物。\n",
    "```python\n",
    "class Animal:\n",
    "    def walk(self):\n",
    "        print(\"Animal is walking\")\n",
    "    def eat(self):\n",
    "        print(\"Animal is eating\")\n",
    "class Person(Animal):\n",
    "    pass\n",
    "p1=Person()\n",
    "p1.walk()   #Animal is walking\n",
    "```\n",
    "组合的方式\n",
    "用组合的方式建立了类与组合的类之间的关系，它是一种‘有’的关系,比如老师有生日，老师教python课程。\n",
    "```python\n",
    "class Teacher:\n",
    "    def __init__(self,name,sex,course):\n",
    "        self.name=name\n",
    "        self.sex=sex\n",
    "        self.course = course\n",
    "class Course:\n",
    "    def __init__(self, name, period):\n",
    "        self.name = name\n",
    "        self.period = period\n",
    "co=Course(\"python\",\"7 m\")\n",
    "t1=Teacher(\"zhang\",\"male\",co)\n",
    "print(t1.course.name)\n",
    "```\n",
    "结果：\n",
    "\n",
    "python\n",
    "\n",
    "**当类之间有显著不同，并且较小的类是较大的类所需要的组件时，用组合比较好。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n"
     ]
    }
   ],
   "source": [
    "# 写法3（通过实例访问）\n",
    "class Teacher:\n",
    "    def __init__(self,name,sex,course):\n",
    "        self.name=name\n",
    "        self.sex=sex\n",
    "        self.course = course\n",
    "class Course:\n",
    "    def __init__(self, name, period):\n",
    "        self.name = name\n",
    "        self.period = period\n",
    "co=Course(\"python\",\"7 m\")\n",
    "t1=Teacher(\"zhang\",\"male\",co)\n",
    "print(t1.course.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python 类的继承和组合\n",
    "https://blog.csdn.net/mpu_nice/article/details/108054838\n",
    "\n",
    "Python面向对象之组合\n",
    "https://www.cnblogs.com/Lea4ning/p/17961558\n",
    "\n",
    "Python_抽象方法——@abc.abstractmethod的使用与解释\n",
    "https://blog.csdn.net/qq_59344127/article/details/131004865\n",
    "\n",
    "python的类的组合\n",
    "https://blog.csdn.net/qq_36594235/article/details/110728933"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NaN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_63528\\2171402674.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s用%s 攻击%s ,%s 掉了 %s血，还剩%s血\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobj1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mgamerole1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGamerole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"role1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mgamerole2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGamerole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"role2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_63528\\2171402674.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, ad, hp)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwea\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaN\u001b[0m  \u001b[1;31m#[] #''# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mattack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NaN' is not defined"
     ]
    }
   ],
   "source": [
    "# 要求：\n",
    "# （1）：创建一个任务角色类Gamerole，构造方法中封装三个属性：name,ad（攻击力），hp（血量）\n",
    "# （2）：Gamerole类中定义一个方法attack：实例化两个对象以及互相攻击的功能\n",
    "# （3）：创建一个工具类Weapon，构造方法中封装三个属性：name,ad（攻击力）\n",
    "# （4）：Weapon类中定义一个方法fight：实例化工具对象以及攻击的力度\n",
    "\n",
    "class Gamerole:\n",
    "    def __init__(self,name,ad,hp):\n",
    "        self.name = name\n",
    "        self.ad = ad\n",
    "        self.hp = hp\n",
    "#         self.wea = #[] #''# None 可以先把self.wea定义为空值，也可以不用\n",
    "#         最后还是可以用到equipment函数定义的wea变量\n",
    "    def attack(self,obj):\n",
    "        obj.hp = obj.hp -self.ad\n",
    "        print(\"%s 攻击%s ,%s 掉了 %s血，还剩%s血\" % (self.name,obj.name,obj.name,self.ad,obj.hp))\n",
    "\n",
    "    def equipment(self,wea):\n",
    "        self.wea = wea\n",
    "class Weapon:\n",
    "    def __init__(self,name,ad):\n",
    "        self.name = name\n",
    "        self.ad = ad\n",
    "\n",
    "    def fight(self,obj1,obj2):\n",
    "        obj2.hp = obj2.hp - self.ad\n",
    "        print(\"%s用%s 攻击%s ,%s 掉了 %s血，还剩%s血\" % (obj1.name, self.name,obj2.name, obj2.name, self.ad, obj2.hp))\n",
    "gamerole1 = Gamerole(\"role1\",30,200)\n",
    "gamerole2 = Gamerole(\"role2\",20,100)\n",
    "\n",
    "weapon1 = Weapon(\"刀\",10)\n",
    "weapon2 = Weapon(\"剑\",20)\n",
    "\n",
    "weapon1.fight(gamerole1,gamerole2)#方案1\n",
    "\n",
    "# 通过实例传递属性\n",
    "gamerole1.equipment(weapon1)\n",
    "gamerole1.wea.fight(gamerole1,gamerole2)#方案2\n",
    "\n",
    "# 方案一 和方案2 对比：\n",
    "# 方案1使用的是非组合的形式，攻击的发起者是工具，\n",
    "# 不符合人们的一个习惯，攻击的发起者应该是人，\n",
    "# 故方案二使用了类的组合的形式，\n",
    "# 其中对象gamerole1封装了一个属性weapon1，\n",
    "# weapon1是另一个类Weapon的一个实例，\n",
    "# 很好的解决了攻击的发起者是人的这个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stack和cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n",
      "torch.Size([3, 2, 3])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_508\\3423974503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m# outputs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "# stack沿着一个新维度对输入张量序列进行连接。 \n",
    "# 序列中所有的张量都应该为相同形状。\n",
    "\n",
    "import torch\n",
    "# 假设是时间步T1的输出\n",
    "T1 = torch.tensor([[1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]])\n",
    "# 假设是时间步T2的输出\n",
    "T2 = torch.tensor([[10, 20, 30],\n",
    "        [40, 50, 60],\n",
    "        [70, 80, 90]])\n",
    "print(torch.stack((T1,T2),dim=0).shape)\n",
    "print(torch.stack((T1,T2),dim=1).shape)\n",
    "print(torch.stack((T1,T2),dim=2).shape)\n",
    "print(torch.stack((T1,T2),dim=3).shape)\n",
    "# outputs:\n",
    "torch.Size([2, 3, 3])\n",
    "torch.Size([3, 2, 3])\n",
    "torch.Size([3, 3, 2])\n",
    "'选择的dim>len(outputs)，所以报错'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[11, 21, 31],\n",
      "        [21, 31, 41]], dtype=torch.int32), tensor([[12, 22, 32],\n",
      "        [22, 32, 42]], dtype=torch.int32)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_508\\3988114553.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;34m'打印查看'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m [tensor([[11, 21, 31],\n\u001b[0m\u001b[0;32m     13\u001b[0m          [21, 31, 41]], dtype=torch.int32),\n\u001b[0;32m     14\u001b[0m  tensor([[12, 22, 32],\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# cat在现有维度上拼接输入的张量序列\n",
    "# x1\n",
    "x1 = torch.tensor([[11,21,31],[21,31,41]],dtype=torch.int)\n",
    "x1.shape # torch.Size([2, 3])\n",
    "# x2\n",
    "x2 = torch.tensor([[12,22,32],[22,32,42]],dtype=torch.int)\n",
    "x2.shape  # torch.Size([2, 3])\n",
    "'inputs为２个形状为[2 , 3]的矩阵 '\n",
    "inputs = [x1, x2]\n",
    "print(inputs)\n",
    "'打印查看'\n",
    "[tensor([[11, 21, 31],\n",
    "         [21, 31, 41]], dtype=torch.int32),\n",
    " tensor([[12, 22, 32],\n",
    "         [22, 32, 42]], dtype=torch.int32)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pettingzoo（环境用37）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1907757 , 0.7369741 , 0.28813577, 0.49463144, 0.09961204],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试petting zoo是否可用\n",
    "\n",
    "from pettingzoo.mpe import simple_spread_v3\n",
    "# discrete env\n",
    "dis_env = simple_spread_v3.parallel_env(N=3, continuous_actions=False)\n",
    "# continuous env\n",
    "con_env = simple_spread_v3.parallel_env(N=3, continuous_actions=True)\n",
    "dis_env.reset()\n",
    "con_env.reset()\n",
    "dis_env.action_space('agent_0').sample() # 2\n",
    "con_env.action_space('agent_0').sample() # array([0.24120373, 0.83279127, 0.4586939 , 0.4208583 , 0.97381055], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪刀石头布测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: ROCK\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: ROCK\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: PAPER\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: PAPER\n",
      "Current state: Agent1: SCISSORS , Agent2: None\n",
      "Current state: Agent1: SCISSORS , Agent2: SCISSORS\n",
      "Current state: Agent1: PAPER , Agent2: None\n",
      "Current state: Agent1: PAPER , Agent2: ROCK\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n",
      "Current state: Agent1: ROCK , Agent2: None\n",
      "Current state: Agent1: ROCK , Agent2: SCISSORS\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "from gymnasium.spaces import Discrete\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "\n",
    "ROCK = 0\n",
    "PAPER = 1\n",
    "SCISSORS = 2\n",
    "NONE = 3\n",
    "MOVES = [\"ROCK\", \"PAPER\", \"SCISSORS\", \"None\"]\n",
    "NUM_ITERS = 100\n",
    "REWARD_MAP = {\n",
    "    (ROCK, ROCK): (0, 0),\n",
    "    (ROCK, PAPER): (-1, 1),\n",
    "    (ROCK, SCISSORS): (1, -1),\n",
    "    (PAPER, ROCK): (1, -1),\n",
    "    (PAPER, PAPER): (0, 0),\n",
    "    (PAPER, SCISSORS): (-1, 1),\n",
    "    (SCISSORS, ROCK): (-1, 1),\n",
    "    (SCISSORS, PAPER): (1, -1),\n",
    "    (SCISSORS, SCISSORS): (0, 0),\n",
    "}\n",
    "\n",
    "\n",
    "def env(render_mode=None):\n",
    "    \"\"\"\n",
    "    The env function often wraps the environment in wrappers by default.\n",
    "    You can find full documentation for these methods\n",
    "    elsewhere in the developer documentation.\n",
    "    \"\"\"\n",
    "    internal_render_mode = render_mode if render_mode != \"ansi\" else \"human\"\n",
    "    env = raw_env(render_mode=internal_render_mode)\n",
    "    # This wrapper is only for environments which print results to the terminal\n",
    "    if render_mode == \"ansi\":\n",
    "        env = wrappers.CaptureStdoutWrapper(env)\n",
    "    # this wrapper helps error handling for discrete action spaces\n",
    "    env = wrappers.AssertOutOfBoundsWrapper(env)\n",
    "    # Provides a wide vareity of helpful user errors\n",
    "    # Strongly recommended\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "class raw_env(AECEnv):\n",
    "    \"\"\"\n",
    "    The metadata holds environment constants. From gymnasium, we inherit the \"render_modes\",\n",
    "    metadata which specifies which modes can be put into the render() method.\n",
    "    At least human mode should be supported.\n",
    "    The \"name\" metadata allows the environment to be pretty printed.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": [\"human\"], \"name\": \"rps_v2\"}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        \"\"\"\n",
    "        The init method takes in environment arguments and\n",
    "         should define the following attributes:\n",
    "        - possible_agents\n",
    "        - render_mode\n",
    "\n",
    "        Note: as of v1.18.1, the action_spaces and observation_spaces attributes are deprecated.\n",
    "        Spaces should be defined in the action_space() and observation_space() methods.\n",
    "        If these methods are not overridden, spaces will be inferred from self.observation_spaces/action_spaces, raising a warning.\n",
    "\n",
    "        These attributes should not be changed after initialization.\n",
    "        \"\"\"\n",
    "        self.possible_agents = [\"player_\" + str(r) for r in range(2)]\n",
    "\n",
    "        # optional: a mapping between agent name and ID\n",
    "        self.agent_name_mapping = dict(\n",
    "            zip(self.possible_agents, list(range(len(self.possible_agents))))\n",
    "        )\n",
    "\n",
    "        # optional: we can define the observation and action spaces here as attributes to be used in their corresponding methods\n",
    "        self._action_spaces = {agent: Discrete(3) for agent in self.possible_agents}\n",
    "        self._observation_spaces = {\n",
    "            agent: Discrete(4) for agent in self.possible_agents\n",
    "        }\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "    # Observation space should be defined here.\n",
    "    # lru_cache allows observation and action spaces to be memoized, reducing clock cycles required to get each agent's space.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def observation_space(self, agent):\n",
    "        # gymnasium spaces are defined and documented here: https://gymnasium.farama.org/api/spaces/\n",
    "        return Discrete(4)\n",
    "\n",
    "    # Action space should be defined here.\n",
    "    # If your spaces change over time, remove this line (disable caching).\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        return Discrete(3)\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the environment. In human mode, it can print to terminal, open\n",
    "        up a graphical window, or open up some other display that a human can see and understand.\n",
    "        \"\"\"\n",
    "        if self.render_mode is None:\n",
    "            gymnasium.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode.\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        if len(self.agents) == 2:\n",
    "            string = \"Current state: Agent1: {} , Agent2: {}\".format(\n",
    "                MOVES[self.state[self.agents[0]]], MOVES[self.state[self.agents[1]]]\n",
    "            )\n",
    "        else:\n",
    "            string = \"Game over\"\n",
    "        print(string)\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"\n",
    "        Observe should return the observation of the specified agent. This function\n",
    "        should return a sane observation (though not necessarily the most up to date possible)\n",
    "        at any time after reset() is called.\n",
    "        \"\"\"\n",
    "        # observation of one agent is the previous state of the other\n",
    "        return np.array(self.observations[agent])\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close should release any graphical displays, subprocesses, network connections\n",
    "        or any other environment data which should not be kept around after the\n",
    "        user is no longer using the environment.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset needs to initialize the following attributes\n",
    "        - agents\n",
    "        - rewards\n",
    "        - _cumulative_rewards\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection\n",
    "        And must set up the environment so that render(), step(), and observe()\n",
    "        can be called without issues.\n",
    "        Here it sets up the state dictionary which is used by step() and the observations dictionary which is used by step() and observe()\n",
    "        \"\"\"\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self.state = {agent: NONE for agent in self.agents}\n",
    "        self.observations = {agent: NONE for agent in self.agents}\n",
    "        self.num_moves = 0\n",
    "        \"\"\"\n",
    "        Our agent_selector utility allows easy cyclic stepping through the agents list.\n",
    "        \"\"\"\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        step(action) takes in an action for the current agent (specified by\n",
    "        agent_selection) and needs to update\n",
    "        - rewards\n",
    "        - _cumulative_rewards (accumulating the rewards)\n",
    "        - terminations\n",
    "        - truncations\n",
    "        - infos\n",
    "        - agent_selection (to the next agent)\n",
    "        And any internal state used by observe() or render()\n",
    "        \"\"\"\n",
    "        if (\n",
    "            self.terminations[self.agent_selection]\n",
    "            or self.truncations[self.agent_selection]\n",
    "        ):\n",
    "            # handles stepping an agent which is already dead\n",
    "            # accepts a None action for the one agent, and moves the agent_selection to\n",
    "            # the next dead agent,  or if there are no more dead agents, to the next live agent\n",
    "            self._was_dead_step(action)\n",
    "            return\n",
    "\n",
    "        agent = self.agent_selection\n",
    "\n",
    "        # the agent which stepped last had its _cumulative_rewards accounted for\n",
    "        # (because it was returned by last()), so the _cumulative_rewards for this\n",
    "        # agent should start again at 0\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "\n",
    "        # stores action of current agent\n",
    "        self.state[self.agent_selection] = action\n",
    "\n",
    "        # collect reward if it is the last agent to act\n",
    "        if self._agent_selector.is_last():\n",
    "            # rewards for all agents are placed in the .rewards dictionary\n",
    "            self.rewards[self.agents[0]], self.rewards[self.agents[1]] = REWARD_MAP[\n",
    "                (self.state[self.agents[0]], self.state[self.agents[1]])\n",
    "            ]\n",
    "\n",
    "            self.num_moves += 1\n",
    "            # The truncations dictionary must be updated for all players.\n",
    "            self.truncations = {\n",
    "                agent: self.num_moves >= NUM_ITERS for agent in self.agents\n",
    "            }\n",
    "\n",
    "            # observe the current state\n",
    "            for i in self.agents:\n",
    "                self.observations[i] = self.state[\n",
    "                    self.agents[1 - self.agent_name_mapping[i]]\n",
    "                ]\n",
    "        else:\n",
    "            # necessary so that observe() returns a reasonable observation at all times.\n",
    "            self.state[self.agents[1 - self.agent_name_mapping[agent]]] = NONE\n",
    "            # no rewards are allocated until both players give an action\n",
    "            self._clear_rewards()\n",
    "\n",
    "        # selects the next agent.\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        # Adds .rewards to ._cumulative_rewards\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "            \n",
    "# 运行部分\n",
    "# import aec_rps\n",
    "\n",
    "env1 = env(render_mode=\"human\")\n",
    "env1.reset(seed=42)\n",
    "\n",
    "for agent in env1.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env1.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env1.action_space(agent).sample()\n",
    "\n",
    "    env1.step(action)\n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## enumerate\n",
    "用于自定义可迭代对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n",
      "1 a\n",
      "2 b\n",
      "3 c\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(['a', 'b', 'c']):# 默认start=0\n",
    "    print(index, value)\n",
    "for index, value in enumerate(['a', 'b', 'c'], start=1):# 自定义start\n",
    "    print(index, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparse模块是命令行选项、参数和子命令解析器。可以让人轻松编写用户友好的命令行接口。适用于代码需要频繁地修改参数的情况。以下为实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.42477796076938\n"
     ]
    }
   ],
   "source": [
    "# 没有argparse\n",
    "import math  # 为了获取π\n",
    "def cylinder_volume(radius, height):\n",
    "    vol = (math.pi) * (radius**2) * (height)  # 体积公式\n",
    "    return vol\n",
    "if __name__ == '__main__':\n",
    "    print(cylinder_volume(1, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.42477796076938\n"
     ]
    }
   ],
   "source": [
    "# 有argparse1，这种方法最常用，因为它允许你在cmd中以任何顺序指定参数，并且参数的含义更明确\n",
    "import math\n",
    "import argparse  # 导入argparse模块\n",
    "# 用来装载参数的容器\n",
    "parser = argparse.ArgumentParser(description='Calculate volume of a cylinder')\n",
    "\n",
    "'给这个解析对象添加命令行参数，名称前不加--就是位置参数，必须在命令行里面赋值，加了--是可选参数，可以用默认值'\n",
    "\n",
    "parser.add_argument('--radius', type=int, default=1, help='Radius of cylinder')\n",
    "parser.add_argument('--height', type=int, default=3, help='Height of cylinder')\n",
    "\n",
    "args = parser.parse_args()  # 获取所有参数\n",
    "def cylinder_volume(radius, height):\n",
    "    vol = (math.pi) * (radius**2) * (height)\n",
    "    return vol\n",
    "if __name__ == '__main__':\n",
    "    print(cylinder_volume(args.radius, args.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码在命令行中的用法有\n",
    "`python your_script.py`\n",
    "或者改变参数\n",
    "`python your_script.py --radius 2 --height 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The volume of the cylinder is 9.42\n"
     ]
    }
   ],
   "source": [
    "# 有argparse2，这种方法较少，命令行里面参数必须按顺序输入\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Calculate volume of a cylinder')\n",
    "\n",
    "\"在这个版本中，radius和height仍然是位置参数，但是有了nargs='?' 表示这个参数是可选的。你可以这样运行脚本：\"\n",
    "\n",
    "parser.add_argument('radius', type=int, nargs='?', default=1, help='Radius of cylinder')\n",
    "parser.add_argument('height', type=int, nargs='?', default=3, help='Height of cylinder')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 使用参数\n",
    "radius = args.radius\n",
    "height = args.height\n",
    "\n",
    "volume = math.pi * radius**2 * height\n",
    "print(f\"The volume of the cylinder is {volume:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码在命令行中的用法有\n",
    "`python your_script.py`\n",
    "或者改变参数\n",
    "`python your_script.py 2 4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file\n"
     ]
    }
   ],
   "source": [
    "'argparse提供了简写形式如下'\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Deep Gaussian Processes on MNIST\")\n",
    "parser.add_argument(\"-n\", \"--num-epochs\", default=5, type=int)\n",
    "parser.add_argument(\"-t\", \"--num-iters\", default=60, type=int)\n",
    "parser.add_argument(\"-b\", \"--batch-size\", default=1000, type=int)\n",
    "parser.add_argument(\"-lr\", \"--learning-rate\", default=0.01, type=float)\n",
    " \n",
    "parser.add_argument(\"-f\",\"--file\",default=\"file\")#接收这个-f参数\n",
    "args = parser.parse_args()\n",
    "print(args.file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用这种方式定义参数后，你可以在命令行中用简写方式 `python your_script.py -n 10 -t 100 -b 500 -lr 0.001 -f myfile.txt`或完整形式`python your_script.py --num-epochs 10 --num-iters 100 --batch-size 500 --learning-rate 0.001 --file myfile.txt`指定参数，在代码中，你可以通过 `args.num_epochs、args.num_iters`等方式访问这些参数的值，无论它们是通过简写形式还是完整形式在命令行中指定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1211.2px",
    "left": "43px",
    "top": "134.768px",
    "width": "208px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "146.85px",
    "left": "1083.2px",
    "right": "20px",
    "top": "78px",
    "width": "428.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
