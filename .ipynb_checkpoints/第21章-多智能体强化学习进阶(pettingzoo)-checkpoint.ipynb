{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b637bed2e4b49784",
   "metadata": {
    "id": "b637bed2e4b49784",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Inline Plotting\n",
    "# 实例代码用到的动作空间是离散的，所以用独热编码选择最优动作\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8f4955e1268aa",
   "metadata": {
    "id": "87a8f4955e1268aa",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Multi-Agent Reinforcement Learning Experiment\n",
    "\n",
    "Note: This is an implement of the MADDPG algorithm based on the essay \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835a163e1633f4f",
   "metadata": {
    "id": "a835a163e1633f4f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e48db0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At the very beginning, let's import needed packages. We choose pettingzoo as our environment, installed by using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef0bfb943d38916",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ef0bfb943d38916",
    "outputId": "af96fd11-148e-42d4-c5e3-913634c598dd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Install packages\n",
    "# %pip install pettingzoo[mpe]\n",
    "# %pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaddc110",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll also use the following from PyTorch:\n",
    "\n",
    "- neural networks (torch.nn)\n",
    "- optimization (torch.optim)\n",
    "\n",
    "Matplotlib and Openpyxl are used for saving and visualizing our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85afac75c84429b",
   "metadata": {
    "id": "c85afac75c84429b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "from psutil import virtual_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ffeaa2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Now,let's check the system memory and  NVIDIA Information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8521bc707b30e9f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8521bc707b30e9f",
    "outputId": "b5c522f4-b318-4f99-c02a-aef8441595b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Run in the Colab\n",
    "run_in_colab = False\n",
    "if run_in_colab:\n",
    "    # Check NVIDIA information\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "    if gpu_info.find('failed') >= 0:\n",
    "        print('Not connected to a GPU')\n",
    "    else:\n",
    "        print(gpu_info)\n",
    "    # Check memory information\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "    if ram_gb < 20:\n",
    "        print('Not using a high-RAM runtime')\n",
    "    else:\n",
    "        print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb0b801653d795ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb0b801653d795ca",
    "outputId": "9644f8de-be9f-4726-d840-e69cc39e862b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x1eacf2dff28>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82447ad863fddf",
   "metadata": {
    "id": "82447ad863fddf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e424ef7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Replay Buffer is a method of data storage and retrieval used in Reinforcement Learning. Its principle is to store the agent's experience in the environment in a buffer, and then to randomly extract some of the data from the buffer during training for learning. This has two advantages: first, it improves the utilisation rate of the data and avoids the waste of discarding it only once each time; second, it breaks the temporal correlation of the data and reduces the instability of the training. We chose to implement it with a circular queue of fixed size, and when the queue is full, the oldest data is automatically discarded.\n",
    "\n",
    "Replay Buffer是一种用于强化学习的数据存储和检索方法。其原理是将智能体在环境中的经验存储在缓冲区中，然后在训练时从缓冲区中随机抽取部分数据进行学习。这样做有两个优点：第一，提高了数据的利用率，避免了每次只丢弃一次的浪费；第二，打破了数据的时间相关性，降低了训练的不稳定性。我们选择用固定大小的圆形队列来实现，当队列满时，自动丢弃最旧的数据。\n",
    "\n",
    "For this, we're going to need two classes:\n",
    "\n",
    "- `Transition` - a named tuple representing a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the screen difference image as described later on.一个命名的元组代表我们环境中的单个变迁。它本质上是将(状态,动作)对映射到它们的( next _ state ,奖励)结果，状态是后面描述的屏幕差异图像。\n",
    "- `ReplayBuffer` - a cyclic buffer of bounded size that holds the transitions observed recently. It also implements a `.sample()` method for selecting a random batch of transitions for training. 一个有界大小的循环缓冲区，保存最近观察到的转变。它还实现了一种' . sample ( ) '方法，用于选择随机批次的迁移进行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd698432ed269d7b",
   "metadata": {
    "code_folding": [],
    "id": "dd698432ed269d7b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Transition is a namedtuple used to store a transition.\n",
    "\n",
    "The structure of Transition looks like this:\n",
    "    (state, action, reward, next_state, done)\n",
    "\"\"\"\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"ReplayBuffer is a class used to achieve experience replay.\n",
    "\n",
    "    It's a buffer composed of a deque with a certain capacity. When the deque is full, it will automatically remove the oldest transition in the buffer.\n",
    "\n",
    "    Attributes:\n",
    "        _storage: The buffer to store the transitions.\n",
    "\n",
    "    Examples:\n",
    "        A replay buffer structure looks like below:\n",
    "        [\n",
    "            (state_1, action_1, reward_1, next_state_1, done_1),\n",
    "            (state_2, action_2, reward_2, next_state_2, done_2),\n",
    "            ...\n",
    "            (state_n, action_n, reward_n, next_state_n, done_n),\n",
    "        ]\n",
    "        Each tuple is a transition.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity=100_000):\n",
    "        \"\"\"Initial a replay buffer with capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity: Max length of the buffer.\n",
    "        \"\"\"\n",
    "        self._storage = deque([], maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state:          The state of the agents.\n",
    "            action:         The action of the agents.\n",
    "            reward:         The reward of the agents.\n",
    "            next_state:     The next state of the agents.\n",
    "            done:           The termination of the agents.\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "        transition = Transition(state, action, reward, next_state, done)\n",
    "        self._storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of transitions that we want to sample from the buffer.\n",
    "\n",
    "        Returns: A batch of transitions.\n",
    "\n",
    "        Example:\n",
    "            Assuming that batch_size=3, we'll randomly sample 3 transitions from the buffer:\n",
    "                state_batch = (state_1, state_2, state_3)\n",
    "                action_batch = (action_1, action_2, action_3)\n",
    "                reward_batch = (reward_1, reward_2, reward_3)\n",
    "                next_state_batch = (next_state_1, next_state_2, next_state_3)\n",
    "                done_batch = (done_1, done_2, done_3)\n",
    "        \"\"\"\n",
    "        transitions = random.sample(self._storage, batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*transitions)\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the buffer.\"\"\"\n",
    "        return len(self._storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e015a5f56d7876f",
   "metadata": {
    "id": "1e015a5f56d7876f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Simple Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed49e00c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code is a simple neural network class written in the PyTorch framework. It has the following characteristics:\n",
    "\n",
    "- It inherits the `nn.Module` class, which is the base class for all neural network models in PyTorch.继承了' nn。模块类，是PyTorch中所有神经网络模型的基类。\n",
    "- It defines three fully connected layers (`nn.Linear`), which are the input layer, the hidden layer and the output layer. Its parameters are input dimensions, output dimensions and hidden dimensions.定义3个全连接层( ' nn。Linear ' )，分别为输入层、隐含层和输出层。其参数为输入维度、输出维度和隐藏维度。\n",
    "- It defines the `forward` propagation process of the neural network in the forward method. It first passes the input data $x$ through the first layer and applies the ReLU activation function (`F.relu`) to it. It then passes the result through the second layer again and applies the `relu` activation function again. Finally, it passes the result through the third layer and returns the output data.在前向方法中定义了神经网络的\"前向\"传播过程。它首先将输入数据x通过第一层，并将ReLU激活函数( ' F.relu ' )应用到其中。然后将结果再次通过第二层，再次应用\" relu \"激活函数。最后，通过第三层传递结果并返回输出数据。\n",
    "\n",
    "The reasons we use three-layer neural networks are as follows:\n",
    "- Each linear layer introduces additional parameters and nonlinear activation functions, which increase the complexity and flexibility of the model. More linear layers mean that models can learn richer feature representations and better fit complex relationships between inputs and outputs 每个线性层引入了额外的参数和非线性激活函数，增加了模型的复杂性和灵活性。更多的线性层意味着模型可以学习到更丰富的特征表示，更好地拟合输入和输出之间的复杂关系\n",
    "- If the model is too complex and the amount of training data is relatively small, it is prone to overfitting problems. Too many linear layers increase the complexity of the model and make it easier to remember the details of the training sample, ignoring the ability to generalize to new data. 如果模型过于复杂，训练数据量相对较少，容易出现过拟合问题。过多的线性层增加了模型的复杂度，使其更容易记住训练样本的细节，忽略了对新数据的泛化能力。\n",
    "- DQN is based on Q-learning algorithm ，which usually adopts three-layer CNN structure.DQN基于Q - learning算法，通常采用三层CNN结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5729db93ad71867",
   "metadata": {
    "id": "a5729db93ad71867",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"SimpleNet is a 3-layer simple neural network.\n",
    "\n",
    "    It's used to approximate the policy functions and the value functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        \"\"\"Initial a simple network for an actor or a critic.\n",
    "\n",
    "        Args:\n",
    "            input_dim: The input dimension of the network.\n",
    "            output_dim: The output dimension of the network.\n",
    "            hidden_dim: The hidden layer dimension of the network.\n",
    "        \"\"\"\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7c08737dd548f",
   "metadata": {
    "id": "cae7c08737dd548f",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Gumbel Softmax Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b6c7b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The action space of each agent in an MPE environment is discrete. The DDPG algorithm we use must itself make the agent's actions derivable for its strategy parameters, which is true for continuous action space, but not for discrete action space. However, this does not mean that the current task cannot be solved using the MADDPG algorithm, because we can use a method called Gumbel-Softmax to get an approximate sample of the discrete distribution. Below we briefly introduce its principles and provide the implementation code. MPE环境中每个智能体的动作空间是离散的。我们使用的DDPG算法本身必须使智能体的动作对其策略参数是可导的，这对于连续的动作空间是成立的，但对于离散的动作空间则不成立。然而，这并不意味着当前任务无法使用MADDPG算法求解，因为我们可以使用一种称为Gumbel - Softmax的方法来获得离散分布的近似样本。下面我们简要介绍其原理并提供实现代码。\n",
    "\n",
    "Suppose you have a random variable that follows a discrete distribution $K=(a_1,...,a_k)$. Then $a_i\\in[0,1]$ means $P(Z=i)$ and $\\sum_{i = 1}^{k}a_i=1$. If we want to sample according to this distribution $z\\sim K$, we can see that sampling this discrete distribution is not derivable. 假设有一个服从离散分布$K=(a_1,...,a_k)$的随机变量。那么$a_i\\in[0,1]$即$P(Z=i)$和$\\sum_{i = 1}^{k}a_i=1$。如果我们想按照这个分布$z\\sim K$进行抽样，可以看出抽样这个离散分布是不可导的。\n",
    "\n",
    "So is there a way to make discrete sampling controllable? The answer is heavy parameterisation, and we use the Gumbel-Softmax technique. Specifically, we introduce a heavy parameter factor $g_i$, which is a noise sampled from $Gumbel(0，1)$: $$g_i=-log(-logu),u\\sim Uniform(0,1)$$Gumbel Softmax samples can be written as:$$y_i=\\frac{exp((loga_i+g_i)/\\tau)}{\\sum_{j = 1}^{k}exp((loga_j+g_i)/\\tau)},i=1,...,k.$$ 那么有没有一种方法可以使离散采样可控呢?答案是重参数化，我们使用Gumbel - Softmax技术。具体来说，我们引入了一个重参数因子$g_i$，它是一个从$Gumbel(0,1)$采样的噪声：$$g_i=-log(-logu),u\\sim Uniform(0,1)$$Gumbel Softmax样本可以写成：$$y_i=\\frac{exp((loga_i+g_i)/\\tau)}{\\sum_{j = 1}^{k}exp((loga_j+g_i)/\\tau)},i=1,...,k.$$\n",
    "\n",
    "In this case, if the discrete value is calculated by $z=\\arg max_iy_i$, the discrete value is approximately equivalent to the value sampled by the discrete $z\\sim K$. Furthermore, the gradient for $a$ is naturally introduced into the sample result $y$. $\\tau>0$ is called the temperature parameter of the distribution, and by adjusting it one can control how similar the generated Gumbel-Softmax distribution is to the discrete distribution: the smaller $\\tau$, the more the resulting distribution tends to be the result $onehot(\\arg max_iloga_i+g_i)$; the larger $\\tau$, the more the resulting distribution tends to be uniformly distributed. 在这种情况下，如果用$z=\\arg max_iy_i$计算离散值，则离散值近似等价于用离散$z\\sim K$采样的值。此外，对a的梯度自然地引入到样本结果y中。$\\tau>0$称为分布的温度参数，通过调节$\\tau>0$可以控制生成的Gumbel - Softmax分布与离散分布的相似程度：$\\tau$越小，生成的分布越趋向于$onehot(\\arg max_iloga_i+g_i)$；$\\tau$越大，得到的分布越趋于均匀分布。\n",
    "\n",
    "Next, define some of the utility functions that need to be used, including functions related to Gumbel Softmax sampling, which allows DDPG to be applied to discrete action spaces. 接下来，定义一些需要使用的效用函数，包括与Gumbel Softmax采样相关的函数，这使得DDPG可以应用于离散的动作空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337f3cc479b8d87b",
   "metadata": {
    "id": "337f3cc479b8d87b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def trans2onehot(logits, eps=0.01):\n",
    "    \"\"\"Transform the output of the actor network to a one-hot vector.\n",
    "\n",
    "    Args:\n",
    "        logits:     The output value of the actor network.\n",
    "        eps:        The epsilon parameter in the epsilon-greedy algorithm used to choose an action.\n",
    "\n",
    "    Returns: An action in one-hot vector form.\n",
    "\n",
    "    \"\"\"\n",
    "    # Generates a one-hot vector form of the action selected by the actor network.\n",
    "    best_action = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # Generate a one-hot vector form of a random action.\n",
    "    size, num_actions = logits.shape\n",
    "    random_index = np.random.choice(range(num_actions), size=size)\n",
    "    random_actions = torch.eye(num_actions)[[random_index]].to(logits.device)\n",
    "    # Select an action using the epsilon-greedy algorithm.\n",
    "    random_mask = torch.rand(size, device=logits.device) <= eps\n",
    "    selected_action = torch.where(random_mask.view(-1, 1), random_actions, best_action)\n",
    "    return selected_action\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tensor_type=torch.float32):\n",
    "    \"\"\"Sample a Gumbel noise from the Gumbel(0,1) distribution.\"\"\"\n",
    "    U = torch.rand(shape, dtype=tensor_type, requires_grad=False)\n",
    "    gumbel_noise = -torch.log(-torch.log(U + eps) + eps)\n",
    "    return gumbel_noise\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\"Sample from the Gumbel-Softmax distribution.\"\"\"\n",
    "    gumbel_noise = sample_gumbel(logits.shape, tensor_type=logits.dtype).to(logits.device)\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\"Sample from Gumbel-Softmax distribution and discretize it.\n",
    "\n",
    "    By returning a one-hot vector of y_hard, but with a gradient of y, we can get both a discrete action interacting with the environment and a correct inverse gradient.\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = trans2onehot(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9432ad7b1133c07d",
   "metadata": {
    "id": "9432ad7b1133c07d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030e0f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next we will use the deep deterministic policy gradient (DDPG) algorithm, which constructs a deterministic policy that uses gradient ascent to maximize values. DDPG is also an actor-critic algorithm. \n",
    "\n",
    "If the policy is deterministic, it can be denoted by $a=u_\\theta(s)$. Similar to the policy gradient theorem, we can derive the deterministic policy gradient theorem:$$\\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\mathbb{E}_{s \\sim \\nu^{\\pi_{\\beta}}}\\left[\\left.\\nabla_{\\theta} \\mu_{\\theta}(s) \\nabla_{a} Q_{\\omega}^{\\mu}(s, a)\\right|_{a=\\mu_{\\theta}(s)}\\right]$$\n",
    "接下来我们将使用深度确定性策略梯度( DDPG )算法，该算法构造了一个确定性策略，使用梯度上升来最大化值。DDPG也是一种演员评论家算法。\n",
    "\n",
    "如果政策是确定性的，则可以用$a=u_\\theta(s)$表示。类似于政策梯度定理，我们可以推导出确定性的政策梯度定理：$$\\nabla_{\\theta} J\\left(\\pi_{\\theta}\\right)=\\mathbb{E}_{s \\sim \\nu^{\\pi_{\\beta}}}\\left[\\left.\\nabla_{\\theta} \\mu_{\\theta}(s) \\nabla_{a} Q_{\\omega}^{\\mu}(s, a)\\right|_{a=\\mu_{\\theta}(s)}\\right]$$\n",
    "\n",
    "where $\\pi _\\beta$ is the policy used to collect data. We can understand this theorem as follows: suppose there is already a function $Q$ given a state $s$. But since the action space is now infinite, it is not possible to find the action with the largest value of $Q$ by iterating over all actions, so we want to use the policy $u$ to find the action $a$ that maximises the value of $Q(s,a)$: $u(s)=\\arg max_aQ(s,a)$.\n",
    "\n",
    "To get $u$, we first use $Q$ to derive a derivative $\\nabla _\\theta Q(s,u_\\theta (s))$ of $u_\\theta$, which uses the chain rule of the gradient, first for $a$ and then for $\\theta$. The function $Q$ is then maximised by a gradient ascent to get the action with the largest $Q$ value.\n",
    "其中，$\\pi _\\beta$是用来收集数据的政策。我们可以这样理解这个定理：假设给定一个状态s，已经存在一个函数Q。但是由于动作空间现在是无限的，不可能通过迭代所有动作找到Q值最大的动作，所以我们想用策略u找到使$Q(s,a)$：$u(s)=\\arg max_aQ(s,a)$的值最大的动作a。\n",
    "为了得到u，我们首先利用Q推导出$u_\\theta$的一个导数$\\nabla _\\theta Q(s,u_\\theta (s))$，它利用梯度的链式规则，先求出α，再求出$\\theta$。然后通过梯度上升对函数Q进行最大化，得到具有最大Q值的动作。\n",
    "\n",
    "DDPG uses four neural networks, one each for Actor and Critic, and one each for a target network. In DDPG, Actors also need a target net, as the target net is also used to calculate the target value $Q$. The update of the target &Q& network in DDPG is slightly different from that in DQN: in DQN, the $Q$ network is replicated directly to the target $Q$ network at regular intervals; in DDPG, the update of the target $Q$ network uses a soft update method, i.e. the target $Q$ network is updated slowly and gradually approaches the $Q$ network, and its formula is$$w^-\\leftarrow \\tau w+(1-\\tau)w^-$$\n",
    "DDPG使用4个神经网络，Actor和Critic各1个，目标网络各1个。在DDPG中，Actors也需要一个目标网络，因为目标网络也用于计算目标值Q。DDPG中的目标& Q &网络的更新与DQN中的略有不同：在DQN中，Q网络是定期直接复制到目标Q网络的；在DDPG中，目标Q网络的更新采用软更新方式，即目标Q网络缓慢更新并逐渐逼近Q网络，其公式为$$w^- \\leftarrow \\tau w+(1- \\tau)w^- $$\n",
    "\n",
    "Usually $\\tau$ is a relatively small number, and when $\\tau=1$ it is consistent with how the DQN is updated. The target network $u$ also uses this soft update method.\n",
    "\n",
    "In addition, due to the problem of overestimating the $Q$ value of the function $Q$, DDPG uses the technology in Double DQN to update the network. However, because DDPG uses a deterministic strategy, its own exploration is still very limited. Remembering the DQN algorithm, its exploration is mainly generated by the behaviour of the greedy strategy. Also as an algorithm for offline strategies, DDPG introduces a random noise on the behavioural strategies to explore.\n",
    "通常$\\tau$是一个比较小的数，而当$\\tau=1$时它与DQN是如何更新的是一致的。目标网络u也采用这种软更新方式。\n",
    "此外，由于存在高估函数Q值的问题，DDPG采用双层深度Q网络中的技术更新网络。然而，由于DDPG采用的是确定性策略，其本身的探索仍然非常有限。记住DQN算法，它的探索主要是由贪心策略的行为产生的。同样作为离线策略的一种算法，DDPG引入了随机噪声对行为策略进行探索。\n",
    "\n",
    "Both the strategy network and the value network use a single hidden layer neural network. The output layer of the strategy network uses the tangent function $y=\\tanh x$ as the activation function, because the value domain of the tangent function is $[-1,1]$, which is convenient to scale to the range of actions acceptable to the environment. In DDPG, which deals with environments that interact with continuous actions, the input to the x-network is a vector of states and actions concatenated together, and the output of the x-network is a value representing the value of that state-action pair.\n",
    "策略网络和价值网络均采用单隐层神经网络。策略网络的输出层使用正切函数$y=\\tanh x$作为激活函数，因为正切函数的值域为$[- 1,1]$，便于扩展到环境可接受的动作范围。在处理与连续动作交互的环境的DDPG中，对桥接四端网络的输入是状态和动作串联在一起的向量，桥接四端网络的输出是表示该状态-动作对的值。\n",
    "\n",
    "When taking actions with a policy network, we add Gaussian noise to the actions for better exploration. In the original DDPG paper, the added noise corresponds to the Ornstein-Uhlenbeck (OU) stochastic process:$$\\Delta x_{t}=\\theta\\left(\\mu-x_{t-1}\\right)+\\sigma W$$\n",
    "\n",
    "where $u$ is the mean, $W$ is random noise following Brownian motion, and $\\theta$ and $\\sigma$ are scale parameters. It can be seen that $x(t-1)$ when deviating from the mean, the value of $x_t$ moves closer to the mean. the OU stochastic process is characterised by a linear negative feedback around the mean with an additional disturbance term.\n",
    "当使用策略网络采取行动时，我们将高斯噪声添加到行动中，以便更好地进行探索。在原始的DDPG论文中，加入的噪声对应于奥恩斯泰因-于伦贝克( OU )随机过程：$$\\Delta x_{t}=\\theta\\left(\\mu-x_{t- 1}\\right)+\\sigma W$$\n",
    "其中，u为均值，W为服从布朗运动的随机噪声，$\\theta$和$\\sigma$为尺度参数。可以看出，$x(t- 1)$在偏离均值时，$x_t$的值向均值靠拢。OU随机过程的特点是在均值附近有一个线性负反馈，并附加一个扰动项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72c554e4380ea6fb",
   "metadata": {
    "code_folding": [],
    "id": "72c554e4380ea6fb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \"\"\"The DDPG Algorithm.\n",
    "\n",
    "    1. Each instance of DDPG corresponds an agent.\n",
    "    2. Each instance of DDPG consists of an actor (policy network) and a critic (value network).\n",
    "    3. Each instance of DDPG contains a set of target networks for its actor and critic (affected by the double DQN strategy).\n",
    "    4. Network update function is contained in the Center Controller class, the MADDPG class, so that we can achieve the Centralized Training and Decentralized Execution method easily.\n",
    "\n",
    "    Attributes:\n",
    "        actor:              The Actor (Policy Network).\n",
    "        target_actor:       The Target Actor (Target Policy Network).\n",
    "        critic:             The Critic (value Network).\n",
    "        target_critic:      The Target Critic (Target Value Network).\n",
    "        actor_optimizer:    The optimizer of Actor.\n",
    "        critic_optimizer:   The optimizer of Critic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, critic_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        \"\"\"Initialize a DDPG instance for an agent.\n",
    "\n",
    "        Args:\n",
    "            state_dim:      The dimension of the state, which is also the input dimension of Actor and a part of Critics' input.\n",
    "            action_dim:     The dimension of the action, which is also a part of Critics' input.\n",
    "            critic_dim:     The dimension of the Critics' input (Critic Dimension = State Dimensions + Action Dimension).\n",
    "            hidden_dim:     The dimension of the hidden layer of the networks.\n",
    "            actor_lr:       The learning rate for the Actor.\n",
    "            critic_lr:      The learning rate for the Critic.\n",
    "            device:         The device to compute.\n",
    "        \"\"\"\n",
    "        # Set Actor with Target Network\n",
    "        self.actor = SimpleNet(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = SimpleNet(state_dim, action_dim, hidden_dim).to(device)\n",
    "        # Set Critic with Target Network\n",
    "        self.critic = SimpleNet(critic_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = SimpleNet(critic_dim, 1, hidden_dim).to(device)\n",
    "        # Load parameters from Actor and Critic to their target networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        # Set up optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        \"\"\"Take action from the Actor (Policy Network).\n",
    "\n",
    "        1. State -> the Actor -> Action Value\n",
    "        2. Choose action according to the value of explore.\n",
    "            - explore == True: Choose an action based on the gumbel trick.\n",
    "            - explore == False: Choose the action from the Actor, and transform the value to a one-hot vector.\n",
    "\n",
    "        Args:\n",
    "            state:      The partial observation of the agent.\n",
    "            explore:    The strategy to choose action.\n",
    "\n",
    "        Returns: The action that the Actor has chosen.\n",
    "\n",
    "        \"\"\"\n",
    "        # Choose an action from actor network (deterministic policy network).\n",
    "        action = self.actor(state)\n",
    "        # Exploration and Exploitation\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = trans2onehot(action)\n",
    "        # TODO: Find out why the action need to be transferred to the CPU\n",
    "        # return action.detach().cpu().numpy()[0]\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(net, target_net, tau):\n",
    "        \"\"\"Soft update function, which is used to update the parameters in the target network.\n",
    "\n",
    "        Args:\n",
    "            net:            The original network.\n",
    "            target_net:     The target network.\n",
    "            tau:            Soft update parameter.\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        \"\"\"\n",
    "        # Update target network's parameters using soft update strategy\n",
    "        for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6bd6a4a3305e3",
   "metadata": {
    "id": "26f6bd6a4a3305e3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. MADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83228c56",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The multi-agent DDPG algorithm literally means an algorithm that implements a DDPG for each agent. All agents share a centralised critic network that simultaneously guides each agent's actor network during the training process, and each agent's actor network acts completely independently, i.e. decentralised execution.多智能体DDPG算法的字面意思是指为每个智能体实现一个DDPG的算法。所有智能体共享一个中心化的评价网络，在训练过程中同时指导每个智能体的行动者网络，并且每个智能体的行动者网络完全独立地行动，即去中心化执行。\n",
    "\n",
    "The main detail of the MADDPG algorithm is that each agent is trained with the Actor-Critic method, but unlike the traditional single agent, the Critic part of each agent in MADDPG can obtain the strategy information of other agents. Specifically, if we consider a game with $N$ agents, each agent's policy parameter is $\\theta=\\lbrace \\theta_1,...,\\theta _N \\rbrace$, and $\\pi=\\lbrace \\pi_1,...,\\pi _N \\rbrace$ is the set of strategies for all agents, then we can write the policy gradient of each agent's expected return in the case of a random strategy:$$\\nabla_{\\theta_{i}} J\\left(\\theta_{i}\\right)=\\mathbb{E}_{s \\sim p^{\\mu}, a \\sim \\pi_{i}}\\left[\\nabla_{\\theta_{i}} \\log \\pi_{i}\\left(a_{i} \\mid o_{i}\\right) Q_{i}^{\\pi}\\left(\\mathbf{x}, a_{1}, \\ldots, a_{N}\\right)\\right]$$ MADDPG算法的主要细节是每个Agent都用Actor - Critic方法进行训练，但与传统的单个Agent不同，MADDPG中每个Agent的Critic部分可以获得其他Agent的策略信息。具体来说，如果我们考虑一个含有N个代理人的博弈，每个代理人的策略参数为$\\theta=\\lbrace \\theta_1,...,\\theta _N \\rbrace$，$\\pi=\\lbrace \\pi_1,...,\\pi _N \\rbrace$是所有代理人的策略集合，那么我们可以写出在随机策略的情况下每个代理人期望收益的策略梯度：$$\\nabla_{\\theta_{i}} J\\left(\\theta_{i}\\right)=\\mathbb{E}_{s \\sim p^{\\mu}, a \\sim \\pi_{i}}\\left[\\nabla_{\\theta_{i}} \\log \\pi_{i}\\left(a_{i} \\mid o_{i}\\right) Q_{i}^{\\pi}\\left(\\mathbf{x}, a_{1}, \\ldots, a_{N}\\right)\\right]$$\n",
    "\n",
    "$Q_i^\\pi=(\\mathbf{x},a_1,...,a_N)$ is a centralised action value function. Why is $Q_i$ a centralised action value function? In general, $\\mathbf{x}=(o_1,...,o_N)$ contains the observations of all agents, and $Q_i$ must also contain the actions of all agents at that moment, so the premise of $Q_i$ is that all agents must give their own observations and corresponding actions at the same time. $Q_i^\\pi=(\\mathbf{x},a_1,...,a_N)$是一个中心化的动作值函数。为什么$Q_i$是一个中心化的动作价值函数?一般而言，$\\mathbf{x}=(o_1,...,o_N)$包含所有智能体的观测值，$Q_i$也必须包含所有智能体在该时刻的动作，因此$Q_i$的前提是所有智能体必须同时给出自己的观测值和相应的动作。\n",
    "\n",
    "For a deterministic strategy, considering that there are now $N$ successive strategies $\\mu_{\\theta_{i}}$, we can get the gradient formula for DDPG:$$\\nabla_{\\theta_{i}} J\\left(\\theta_{i}\\right)=\\mathbb{E}_{s \\sim p^{\\mu}, a \\sim \\pi_{i}}\\left[\\nabla_{\\theta_{i}} \\log \\pi_{i}\\left(a_{i} \\mid o_{i}\\right) Q_{i}^{\\pi}\\left(\\mathbf{x}, a_{1}, \\ldots, a_{N}\\right)\\right]\\nabla_{\\theta_{i}} J\\left(\\mu_{i}\\right)=\\mathbb{E}_{\\mathbf{x} \\sim \\mathcal{D}}\\left[\\left.\\nabla_{\\theta_{i}} \\mu_{i}\\left(o_{i}\\right) \\nabla_{a_{i}} Q_{i}^{\\mu}\\left(\\mathbf{x}, a_{1}, \\ldots, a_{N}\\right)\\right|_{a_{i}=\\mu_{i}\\left(o_{i}\\right)}\\right]$$\n",
    "\n",
    "$D$ is the empirical playback pool we use to store data, and every piece of data it stores is $(\\mathbf{x},\\mathbf{x}',a_1,...,a_N,r_1,...,r_N)$. In MADDPG, the centralized action value function can be updated with the following loss function:$$\\mathcal{L}\\left(\\omega_{i}\\right)=\\mathbb{E}_{\\mathbf{x}, a, r, \\mathbf{x}^{\\prime}}\\left[\\left(Q_{i}^{\\mu}\\left(\\mathbf{x}, a_{1}, \\ldots, a_{N}\\right)-y\\right)^{2}\\right], \\quad y=r_{i}+\\left.\\gamma Q_{i}^{\\mu^{\\prime}}\\left(\\mathbf{x}^{\\prime}, a_{1}^{\\prime}, \\ldots, a_{N}^{\\prime}\\right)\\right|_{a_{j}^{\\prime}=\\mu_{j}^{\\prime}\\left(o_{j}\\right)}$$\n",
    "\n",
    "$\\mu^{\\prime}=\\left(\\mu_{\\theta_{1}}^{\\prime}, \\ldots, \\mu_{\\theta_{N}}^{\\prime}\\right)$ is the set of target policies used in the update value function, which have parameters for deferred updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff8104372bbaafe",
   "metadata": {
    "code_folding": [],
    "id": "4ff8104372bbaafe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    \"\"\"The Multi-Agent DDPG Algorithm.\n",
    "\n",
    "    1. The instance of MADDPG is the Center Controller in the algorithm.\n",
    "    2. The instance of MADDPG contains a list of DDPG instances, which are corresponded with the agents in the environment one by one.\n",
    "\n",
    "    Attributes:\n",
    "        agents:     A list of DDPG instances, which are corresponded with the agents in the environment one by one.\n",
    "        device:     The device to compute.\n",
    "        gamma:      The gamma parameter in TD target.\n",
    "        tau:        The tau parameter for soft update.\n",
    "        critic_criterion: The loss function for the Critic networks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dims, action_dims, critic_dim, hidden_dim, actor_lr, critic_lr, device, gamma, tau):\n",
    "        \"\"\"Initialize a MADDPG instance as the Center Controller.\n",
    "\n",
    "        Args:\n",
    "            state_dims: A list of dimensions of each agent's observation.\n",
    "            action_dims: A list of dimensions of each agent's action.\n",
    "            critic_dim: The dimension of the Critic networks' input.\n",
    "            hidden_dim: The dimension of the networks' hidden layers.\n",
    "            actor_lr: The learning rate for the Actor.\n",
    "            critic_lr: The learning rate for the Critic.\n",
    "            device: The device to compute.\n",
    "            gamma: The gamma parameter in TD target.\n",
    "            tau: The tau parameter for soft update.\n",
    "        \"\"\"\n",
    "        # TODO: Should we use dict to combine the DDPG instance with agents?\n",
    "        self.agents = [\n",
    "            DDPG(state_dim, action_dim, critic_dim, hidden_dim, actor_lr, critic_lr, device)\n",
    "            for state_dim, action_dim in zip(state_dims, action_dims)\n",
    "        ]\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.critic_criterion = nn.MSELoss()\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        \"\"\"A list of Actors for the agents.\"\"\"\n",
    "        return [agent.actor for agent in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        \"\"\"A list of Target Actors for the agents.\"\"\"\n",
    "        return [agent.target_actor for agent in self.agents]\n",
    "\n",
    "    def take_action(self, states, explore):\n",
    "        \"\"\"Take actions from the Actors (Policy Networks).\n",
    "\n",
    "        Args:\n",
    "            states: A list of observations from all the agents.\n",
    "            explore: The strategy to choose action (Exploration or Exploitation).\n",
    "\n",
    "        Returns: A list of actions in one-hot vector form.\n",
    "\n",
    "        \"\"\"\n",
    "        states = [torch.tensor(np.array([state]), dtype=torch.float, device=self.device) for state in states]\n",
    "        return [agent.take_action(state, explore) for agent, state in zip(self.agents, states)]\n",
    "\n",
    "    def update(self, sample, agent_idx):\n",
    "        \"\"\"Update parameters for the agent whose index is agent_idx.\n",
    "\n",
    "        Args:\n",
    "            sample: A batch of transitions used to update all the parameters.\n",
    "            agent_idx: The index of the agent whose Actor and Critic would be updated in this function.\n",
    "\n",
    "        Returns: None\n",
    "\n",
    "        Process:\n",
    "            Parse Data from Sample to Observation, Action, Reward, Next Observation and Done Tag.\n",
    "            Set up Current Agent\n",
    "            Update Current Critic (Value Network) with TD Algorithm:\n",
    "                1. Initialize the Gradient to Zero.\n",
    "                2. Build a Tensor Contains All Target Actions Using Target Actor Networks and Next Observations.\n",
    "                3. Calculate Target Critic Value:\n",
    "                    - Combine Next Observation and Target Action in One To One Correspondence.\n",
    "                    - Calculate Target Critic Value (TD Target).\n",
    "                4. Calculate Critic Value:\n",
    "                    - Combine Observation and Action in One To One Correspondence.\n",
    "                    - Calculate Critic Value (TD Target).\n",
    "                5. Calculate Critic's Loss Using MSELoss Function\n",
    "                6. Backward Propagation.\n",
    "                7. Update Parameters with Gradient Descent.\n",
    "            Update Current Actor (Policy Network) with Deterministic Policy Gradient:\n",
    "                1. Initialize the Gradient to Zero.\n",
    "                2. Get Current Actors' Action in the Shape of One-Hot Vector.\n",
    "                    - Get Current Actor Network Output with Current Observation.\n",
    "                    - Transform the Output into a One-Hot Action Vector.\n",
    "                3. Build the Input of Current Actor's Value Function.\n",
    "                    - Build a tensor that contains all the actions.\n",
    "                    - Combine Observations and Actions in One To One Correspondence.\n",
    "                4. Calculate Actor's Loss Using Critic Network (Value Function) Output.\n",
    "                5. Backward Propagation.\n",
    "                6. Update Parameters with Gradient Descent.\n",
    "\n",
    "        TODO:\n",
    "        1. Why there is a term, \"(1 - dones[agent_idx].view(-1, 1))\", during calculation of TD target?\n",
    "            To take termination in to consideration (To be verified).\n",
    "        2. Why there is a term, \"(current_actor_action_value ** 2).mean() * 1e-3\", during calculation of Actor Loss?\n",
    "            To make the output of the trained Actor more stable and smooth with this regular term (To be verified).\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = sample\n",
    "        current_agent = self.agents[agent_idx]\n",
    "        # Update Current Critic (Value Network) with TD Algorithm\n",
    "        current_agent.critic_optimizer.zero_grad()\n",
    "        # Here is the step to choose the actions from the actor network and we have two strategies.\n",
    "        # Option 1: Use the target network strategy.\n",
    "        target_action = [\n",
    "            trans2onehot(_target_policy(_next_obs))\n",
    "            for _target_policy, _next_obs in zip(self.target_policies, next_states)\n",
    "        ]\n",
    "        # Option 2: Use the double DQN strategy.\n",
    "        # target_action = [\n",
    "        #     # Choose actions from the original network!!!\n",
    "        #     trans2onehot(policy(_next_obs))\n",
    "        #     for policy, _next_obs in zip(self.policies, next_states)\n",
    "        # ]\n",
    "        target_critic_input = torch.cat((*next_states, *target_action), dim=1)\n",
    "        target_critic_value = (rewards[agent_idx].view(-1, 1) +\n",
    "                               self.gamma * current_agent.target_critic(target_critic_input) *\n",
    "                               (1 - dones[agent_idx].view(-1, 1)))\n",
    "        critic_input = torch.cat((*states, *actions), dim=1)\n",
    "        critic_value = current_agent.critic(critic_input)\n",
    "        critic_loss = self.critic_criterion(critic_value, target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        current_agent.critic_optimizer.step()\n",
    "        # Update Current Actor (Policy Network) with Deep Deterministic Policy Gradient\n",
    "        current_agent.actor_optimizer.zero_grad()\n",
    "        current_actor_action_value = current_agent.actor(states[agent_idx])\n",
    "        current_actor_action_onehot = gumbel_softmax(current_actor_action_value)\n",
    "        all_actor_actions = [\n",
    "            current_actor_action_onehot if i == agent_idx else trans2onehot(_policy(_state))\n",
    "            for i, (_policy, _state) in enumerate(zip(self.policies, states))\n",
    "        ]\n",
    "        current_critic_input = torch.cat((*states, *all_actor_actions), dim=1)\n",
    "        actor_loss = (-current_agent.critic(current_critic_input).mean() +\n",
    "                      (current_actor_action_value ** 2).mean() * 1e-3)\n",
    "        actor_loss.backward()\n",
    "        current_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets_params(self):\n",
    "        \"\"\"Update all Target network's parameters using soft update method.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.soft_update(agent.actor, agent.target_actor, self.tau)\n",
    "            agent.soft_update(agent.critic, agent.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7c54ff4830fb8",
   "metadata": {
    "id": "3ec7c54ff4830fb8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Evaluate Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48052d9b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function takes a MADDPG agent as input and evaluates its performance over a specified number of episodes and episode length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14b583adb0e0113",
   "metadata": {
    "id": "f14b583adb0e0113",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(num_agents, maddpg, num_episode=10, len_episode=25):\n",
    "    \"\"\"Evaluate the strategies for learning, and no exploration is undertaken at this time.\n",
    "\n",
    "    Args:\n",
    "        num_agents: The number of the agents.\n",
    "        maddpg: The Center Controller.\n",
    "        num_episode: The number of episodes.\n",
    "        len_episode: The length of each episode.\n",
    "\n",
    "    Returns: Returns list.\n",
    "\n",
    "    \"\"\"\n",
    "    env = simple_adversary_v3.parallel_env(N=num_agents, max_cycles=25, continuous_actions=False)\n",
    "    env.reset()\n",
    "    returns = np.zeros(env.max_num_agents)\n",
    "    # Create an array returns of zeros with a length equal to the number of agents in the environment.\n",
    "    # This array will store the cumulative returns for each agent.\n",
    "    for episode in range(num_episode):\n",
    "        states_dict, rewards_dict = env.reset()\n",
    "        states = [state for state in states_dict.values()]\n",
    "        for episode_step in range(len_episode):\n",
    "            actions = maddpg.take_action(states, explore=False)\n",
    "            # Take actions using the MADDPG agent (maddpg.take_action) based on the current states. \n",
    "            actions_SN = [np.argmax(onehot) for onehot in actions]\n",
    "            actions_dict = {env.agents[i]: actions_SN[i] for i in range(env.max_num_agents)}\n",
    "            next_states_dict, rewards_dict, terminations_dict, truncations_dict, _ = env.step(actions_dict)\n",
    "            # Take a step in the environment by passing the actions dictionary to env.step.  \n",
    "            rewards = [reward for reward in rewards_dict.values()]\n",
    "            next_states = [next_state for next_state in next_states_dict.values()]\n",
    "            states = next_states\n",
    "            rewards = np.array(rewards)\n",
    "            returns += rewards / num_episode\n",
    "    env.close()\n",
    "    return returns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f288f40c5c2466",
   "metadata": {
    "id": "25f288f40c5c2466",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Auxiliary Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6c454",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The function rearranges the elements in the input list x and converts them into PyTorch tensors on the specified device. 该函数将输入列表x中的元素重新排列，并在指定设备上将其转换为PyTorch张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3792d0073af4d80",
   "metadata": {
    "id": "e3792d0073af4d80",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_rearrange(x, device):\n",
    "    \"\"\"Rearrange the transition in the sample.\"\"\"\n",
    "    rearranged = [[sub_x[i] for sub_x in x] for i in range(len(x[0]))]\n",
    "    return [torch.FloatTensor(np.vstack(attribute)).to(device) for attribute in rearranged]\n",
    "    # np.vstack is used to vertically stack the elements in each sublist before converting them to a tensor.\n",
    "    # The resulting tensors are then converted to the appropriate device (GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e279ba1b492ebc3",
   "metadata": {
    "id": "1e279ba1b492ebc3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. Set up Parameters and Initialize Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4093d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It is time to initialize the necessary components and parameters for training an MADDPG agent in a multi-agent environment using the specified settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab6c59ea86f99f6",
   "metadata": {
    "id": "dab6c59ea86f99f6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up Parameters\n",
    "NUM_EPISODES = 5000\n",
    "LEN_EPISODES = 25  # The maximum length of each episode\n",
    "BUFFER_SIZE = 100_000\n",
    "HIDDEN_DIM = 64  # denotes the dimension of the hidden layers in the actor and critic networks.\n",
    "ACTOR_LR = 1e-2\n",
    "CRITIC_LR = 1e-2\n",
    "GAMMA = 0.95  # determines the discount factor for future rewards.\n",
    "TAU = 1e-2  # sets the soft update coefficient for target network updates.\n",
    "BATCH_SIZE = 1024\n",
    "UPDATE_INTERVAL = 100  # determines the number of steps before performing an update on the networks.\n",
    "MINIMAL_SIZE = 4000  # is a minimum threshold for the replay buffer size before starting the training.\n",
    "# Initialize Environment\n",
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=False)\n",
    "env.reset()\n",
    "# Initialize Replay Buffer\n",
    "buffer = ReplayBuffer(BUFFER_SIZE)  # stores experiences from the environment.\n",
    "# Initialize Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Initialize Return List\n",
    "return_list = []\n",
    "# Initialize Total Step\n",
    "total_step = 0\n",
    "# Initialize MADDPG\n",
    "# 1. Get State and Action Dimensions from the Environment\n",
    "state_dims = [env.observation_space(env.agents[i]).shape[0] for i in range(env.num_agents)]\n",
    "action_dims = [env.action_space(env.agents[i]).n for i in range(env.num_agents)]\n",
    "critic_dim = sum(state_dims) + sum(action_dims)  # calculates the total dimension of the critic network's input\n",
    "# 2. Create Center Controller\n",
    "maddpg = MADDPG(state_dims, action_dims, critic_dim, HIDDEN_DIM, ACTOR_LR, CRITIC_LR, device, GAMMA, TAU)\n",
    "# maddpg is created as an instance of the MADDPG class using the specified dimensions, learning rates and other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da4352adbb6fed",
   "metadata": {
    "id": "2da4352adbb6fed",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 9. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b73794",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code implements the training loop for the MADDPG agent, including taking actions, interacting with the environment, updating the networks, and periodically evaluating the agent's performance.\n",
    "- The take_action method is called on the MADDPG agent, with the argument explore=True. This means that the agent will add exploration noise to its action selection, allowing it to explore different actions instead of always selecting the same action deterministically.\n",
    "- The resulting actions are initially in a one-hot encoded format, representing the probability distribution over the possible actions for each agent.\n",
    "- By using numpy.argmax function, each one-hot encoded action is converted into a discrete action by selecting the index of the highest probability.\n",
    "\n",
    "该代码实现了MADDPG代理的训练循环，包括采取行动、与环境交互、更新网络和定期评估代理的性能。\n",
    " -在MADDPG Agent上调用take _ action方法，参数explore = True。这意味着智能体会在其动作选择中加入探索噪声，允许其探索不同的动作，而不是总是确定性地选择相同的动作。\n",
    " -产生的动作最初是以独热编码的形式出现的，代表了每个Agent在可能的动作上的概率分布。\n",
    " -利用numpy . argmax函数，通过选择概率最大的指标，将每个独热编码动作转换为离散动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf02a977f035468",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "edf02a977f035468",
    "outputId": "c2471308-a137-4560-ca01-b114df0c6315",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, [-31.203747216959638, 6.530676731656475, 6.530676731656475]\n",
      "Episode: 200, [-55.23450011494809, 10.619954416321896, 10.619954416321896]\n",
      "Episode: 300, [-57.67763900941242, 10.402926678363892, 10.402926678363892]\n",
      "Episode: 400, [-25.467122190954374, 6.952320489657769, 6.952320489657769]\n",
      "Episode: 500, [-25.411940917168728, 4.00215797798729, 4.00215797798729]\n",
      "Episode: 600, [-25.876940692610578, -10.735109582112317, -10.735109582112317]\n",
      "Episode: 700, [-20.715413147240813, -3.2210908586513, -3.2210908586513]\n",
      "Episode: 800, [-23.316051105557793, 2.197853734257218, 2.197853734257218]\n",
      "Episode: 900, [-22.18900804815811, 5.940932881428632, 5.940932881428632]\n",
      "Episode: 1000, [-23.572624146056064, 4.37474499119723, 4.37474499119723]\n",
      "Episode: 1100, [-20.292345055658984, 2.46480155760341, 2.46480155760341]\n",
      "Episode: 1200, [-20.921237302323178, 4.040273505044046, 4.040273505044046]\n",
      "Episode: 1300, [-22.50002385506226, 5.90893785661199, 5.90893785661199]\n",
      "Episode: 1400, [-22.86158451077527, 8.255357637685815, 8.255357637685815]\n",
      "Episode: 1500, [-20.101014307886846, 3.3896135762033923, 3.3896135762033923]\n",
      "Episode: 1600, [-20.259227436387622, 5.09046634929619, 5.09046634929619]\n",
      "Episode: 1700, [-18.90749116153028, 5.717125378406519, 5.717125378406519]\n",
      "Episode: 1800, [-21.8437434397838, 6.376614804443656, 6.376614804443656]\n",
      "Episode: 1900, [-21.220705646494938, 5.07620485066828, 5.07620485066828]\n",
      "Episode: 2000, [-20.77320477261982, 6.209205634681709, 6.209205634681709]\n",
      "Episode: 2100, [-18.891839085859527, 5.824993472604422, 5.824993472604422]\n",
      "Episode: 2200, [-18.828216255933796, 5.179834555993813, 5.179834555993813]\n",
      "Episode: 2300, [-20.11985158001841, 5.554051008207918, 5.554051008207918]\n",
      "Episode: 2400, [-20.976801483111895, 7.247182519856659, 7.247182519856659]\n",
      "Episode: 2500, [-18.566588026038296, 4.607962986494666, 4.607962986494666]\n",
      "Episode: 2600, [-17.38384143988713, 4.983074209453564, 4.983074209453564]\n",
      "Episode: 2700, [-18.09153394159464, 6.055581323605071, 6.055581323605071]\n",
      "Episode: 2800, [-17.50623545622759, 5.753281670766954, 5.753281670766954]\n",
      "Episode: 2900, [-17.656741124109256, 6.441819343483589, 6.441819343483589]\n",
      "Episode: 3000, [-15.192882054790894, 3.9908774029816745, 3.9908774029816745]\n",
      "Episode: 3100, [-17.116848086839127, 5.180637553842439, 5.180637553842439]\n",
      "Episode: 3200, [-14.628018628621314, 3.1198514476296553, 3.1198514476296553]\n",
      "Episode: 3300, [-15.767353649199336, 4.162160362524691, 4.162160362524691]\n",
      "Episode: 3400, [-14.169618770406414, 1.5075366206702194, 1.5075366206702194]\n",
      "Episode: 3500, [-15.223289254836377, 3.0362632160947265, 3.0362632160947265]\n",
      "Episode: 3600, [-14.09390012593766, 2.381837588366533, 2.381837588366533]\n",
      "Episode: 3700, [-15.570596448454678, 4.2816715908272425, 4.2816715908272425]\n",
      "Episode: 3800, [-15.287190974772864, 3.5889703518109815, 3.5889703518109815]\n",
      "Episode: 3900, [-13.501970665778382, 0.5907150216602993, 0.5907150216602993]\n",
      "Episode: 4000, [-14.183679138324337, 2.1337782935549936, 2.1337782935549936]\n",
      "Episode: 4100, [-15.040929990527728, 3.9312307879630417, 3.9312307879630417]\n",
      "Episode: 4200, [-14.279286450436881, 2.9213905556697357, 2.9213905556697357]\n",
      "Episode: 4300, [-15.289178734240787, 3.7300757122323525, 3.7300757122323525]\n",
      "Episode: 4400, [-15.364826813979787, 3.797156272267962, 3.797156272267962]\n",
      "Episode: 4500, [-14.256932202618014, 3.275703406430968, 3.275703406430968]\n",
      "Episode: 4600, [-14.829296610040709, 3.210178701511761, 3.210178701511761]\n",
      "Episode: 4700, [-14.809063510402897, 4.187770511309486, 4.187770511309486]\n",
      "Episode: 4800, [-14.794059168051882, 3.4038890216583724, 3.4038890216583724]\n",
      "Episode: 4900, [-15.331381389135851, 2.8132864006660014, 2.8132864006660014]\n",
      "Episode: 5000, [-15.214413606079058, 3.4514089641886265, 3.4514089641886265]\n"
     ]
    }
   ],
   "source": [
    "for episode in range(NUM_EPISODES):\n",
    "    # Reset the Environment\n",
    "    states_dict, _ = env.reset()\n",
    "    states = [state for state in states_dict.values()]\n",
    "    # Start a New Game\n",
    "    for episode_step in range(LEN_EPISODES):\n",
    "        # Initial States and Actions\n",
    "        actions = maddpg.take_action(states, explore=True)\n",
    "        actions_SN = [np.argmax(onehot) for onehot in actions]\n",
    "        actions_dict = {env.agents[i]: actions_SN[i] for i in range(env.max_num_agents)}\n",
    "        # Step\n",
    "        next_states_dict, rewards_dict, terminations_dict, truncations_dict, _ = env.step(actions_dict)\n",
    "        # Add to buffer\n",
    "        rewards = [reward for reward in rewards_dict.values()]\n",
    "        next_states = [next_state for next_state in next_states_dict.values()]\n",
    "        terminations = [termination for termination in terminations_dict.values()]\n",
    "        buffer.add(states, actions, rewards, next_states, terminations)\n",
    "        # Update States\n",
    "        states = next_states\n",
    "        # Count += 1\n",
    "        total_step += 1\n",
    "        # When the replay buffer reaches a certain size and the number of steps reaches the specified update interval\n",
    "        # 1. Sample from the replay buffer.\n",
    "        # 2. Update Actors and Critics.\n",
    "        # 3. Update Target networks' parameters.\n",
    "        if len(buffer) >= MINIMAL_SIZE and total_step % UPDATE_INTERVAL == 0:\n",
    "            sample = buffer.sample(BATCH_SIZE)\n",
    "            sample = [sample_rearrange(x, device) for x in sample]\n",
    "            # Update Actors and Critics\n",
    "            for agent_idx in range(env.max_num_agents):\n",
    "                maddpg.update(sample, agent_idx)\n",
    "            # Update Target Parameters\n",
    "            maddpg.update_all_targets_params()\n",
    "    # After every 100 rounds, use the evaluate function to evaluate the trained agents, get the reward list ep_returns, and add it to return_list.\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        episodes_returns = evaluate(env.max_num_agents - 1, maddpg, num_episode=100)\n",
    "        return_list.append(episodes_returns)\n",
    "        print(f\"Episode: {episode + 1}, {episodes_returns}\")\n",
    "# Close the Environment\n",
    "env.close()\n",
    "return_array = np.array(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c2e27aeb4cc35",
   "metadata": {
    "id": "d2c2e27aeb4cc35",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 10. Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846e3f8e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It's time to save and visualize our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e21c1cb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "e21c1cb0",
    "outputId": "014fa2ef-a880-4f8e-e42b-23b90ef6a38b",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_agents = return_array.shape[1]\n",
    "df = pd.DataFrame(return_array, columns=[f'agent_{i - 1}' if i > 0 else 'adversary_0' for i in range(num_agents)])\n",
    "df.to_excel('return_data.xlsx', index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Return')\n",
    "plt.title('Return vs Episodes for each Agent')\n",
    "plt.grid(True)\n",
    "\n",
    "for agent in range(num_agents):\n",
    "    agent_returns = return_array[:, agent]\n",
    "    if agent == 0:\n",
    "        plt.plot(range(100, len(agent_returns) * 100 + 100, 100), return_array[:, 0], label=f'adversary_0')\n",
    "    else:\n",
    "        plt.plot(range(100, len(agent_returns) * 100 + 100, 100), agent_returns, label=f'agent_{agent - 1}')\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig('return_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c3705",
   "metadata": {},
   "source": [
    "## 11. 训练参数存档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最好对着pycharm里面的变量视图查看\n",
    "torch.save(maddpg.agents[0].actor.state_dict(),\"maddpgzoo0.pth\")\n",
    "torch.save(maddpg.agents[1].actor.state_dict(),\"maddpgzoo1.pth\")\n",
    "torch.save(maddpg.agents[2].actor.state_dict(),\"maddpgzoo2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9da35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 12. 可视化运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb883a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=False, render_mode=\"human\" )\n",
    "# Reset the Environment\n",
    "states_dict, _ = env.reset()\n",
    "states = [state for state in states_dict.values()]\n",
    "# Start a New Game\n",
    "for episode_step in range(LEN_EPISODES):\n",
    "    # Initial States and Actions\n",
    "    actions = maddpg.take_action(states, explore=False)\n",
    "    actions_SN = [np.argmax(onehot) for onehot in actions]\n",
    "    actions_dict = {env.agents[i]: actions_SN[i] for i in range(env.max_num_agents)}\n",
    "    # Step\n",
    "    next_states_dict, rewards_dict, terminations_dict, truncations_dict, _ = env.step(actions_dict)\n",
    "    # Add to buffer\n",
    "    rewards = [reward for reward in rewards_dict.values()]\n",
    "    next_states = [next_state for next_state in next_states_dict.values()]\n",
    "    terminations = [termination for termination in terminations_dict.values()]\n",
    "    buffer.add(states, actions, rewards, next_states, terminations)\n",
    "    # Update States\n",
    "    states = next_states\n",
    "    time.sleep(0.08)\n",
    "# Close the Environment\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748424fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae20e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.45px",
    "left": "1183px",
    "right": "20px",
    "top": "7px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
